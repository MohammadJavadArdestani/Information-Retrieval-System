{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import datetime\n",
    "import collections\n",
    "import math\n",
    "import heapq\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## marks lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_marks = [')','(','>','<',\"Ø›\",\"ØŒ\",'{','}',\"ØŸ\",':',\"-\", 'Â»', '\"', 'Â«', '[', ']','\"','+','=','?']\n",
    "marks = ['/','//', '\\\\','|','!', '%', '&','*','$', '#','ØŸ', '*','.','_' ]\n",
    "alphabet_string_lower = string.ascii_lowercase\n",
    "alphabet_string_upper = string.ascii_uppercase\n",
    "english_char =  list(alphabet_string_lower) + list(alphabet_string_upper)\n",
    "\n",
    "\n",
    "sep_list = [\" \", '\\xad', '\\u200e','\\u200f', '\\u200d', '\\u200d', '\\u200d'] + marks\n",
    "\n",
    "# stop_words1 =['Ø¨Ù‡', 'Ùˆ', 'Ø¯Ø±', 'Ø¨Ø§', 'Ø§ÛŒÙ†', 'Ø´Ø¯', 'Ø±Ø§', 'Ú©Ù‡', 'Ø§Ø²', 'Ú©Ù‡', 'Ø§ÛŒÙ†', 'Ø¨Ø§', 'Ø§Ø³Øª', 'Ø¨Ø±Ø§ÛŒ', 'Ø¢Ù†', 'ÛŒÚ©', 'Ø®ÙˆØ¯', 'ØªØ§', 'Ú©Ø±Ø¯', 'Ø¨Ø±', 'Ù‡Ù…', 'Ù†ÛŒØ²', 'Ù‡Ø²Ø§Ø±', 'Ø±ÛŒØ§Ù„', 'Ø¨ÙˆØ¯']\n",
    "stop_words = [\"Ø¨Ø±Ø§ÛŒ\", \"Ù¾Ø³\", \"Ø³Ù¾Ø³\", \"ØªØ§\", \"Ø§Ø²\", \"Ú©Ù‡\", \"Ùˆ\", \"Ø¨Ù‡\", \"Ø±Ø§\", \"Ø¨Ø§\", \"Ø¨Ø±\", \"Ø¯Ø±\", \"Ø§ÛŒÙ†Ú©Ù‡\", \"Ø§ÛŒÙ†\", \"Ø§Ù†\", \"Ú†Ø±Ø§\",\n",
    "              \"Ø´Ø§ÛŒØ¯\", \"Ø§Ù†Ù‡Ø§\", \"Ú†ÙˆÙ†\", \"Ø§Ù†Ø·ÙˆØ±\", \"Ø§ÛŒÙ†Ø·ÙˆØ±\", \"Ø§Ù†Ú†Ù‡\", \"Ø¢Ø®\", \"Ø¢Ø®Ø±\", \"Ø¢Ø®Ø±Ù‡Ø§\", \"Ø§Ø®Ù‡\", \"Ø¢Ø±Ù‡\", \"Ø¢Ø±ÛŒ\", \"Ø§Ù†Ø§Ù†\",\n",
    "              \"Ø§Ú¯Ø±\", \"Ø¯Ø±Ø¨Ø§Ø±Ù‡\", \"Ø®ÛŒÙ„ÛŒ\", \"ØªÙˆÛŒ\", \"Ø¨Ù„Ú©Ù‡\", \"Ø¨Ø¹Ø¶ÛŒ\", \"Ø¨Ø¹Ø¯Ø§\", \"Ø¨Ø§ÛŒØ¯\", \"Ø§Ù„Ø¨ØªÙ‡\", \"Ø§Ù„Ø§Ù†\", \"Ø§ØµÙ„Ø§\", \"Ø§Ø³Ø§Ø³Ø§\",\n",
    "              \"Ø§Ø­ØªÙ…Ø§Ù„Ø§\", \"Ø§Ø®ÛŒØ±Ø§\", \"Ø§ÛŒØ§\", \"Ø§Ù‡Ø§Ù†\", \"Ø§Ù…Ø±Ø§Ù†Ù‡\", \"Ø§Ù† Ú¯Ø§Ù‡\", \"Ø§Ù†Ø±Ø§\", \"Ø§Ù†Ù‚Ø¯Ø±\", \"Ø§Ø³Ø§Ø³Ø§\", \"Ø¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ†\", \"Ù‡Ø±Ø­Ø§Ù„\",\n",
    "              \"Ø¨ÛŒâ€ŒØ§Ù†Ú©Ù‡\", \"Ø¨Ù‡â€ŒÙ‚Ø¯Ø±ÛŒ\", \"Ø¨ÛŒØ´ØªØ±\", \"Ú©Ù…ØªØ±\", \"ØªØ±\", \"ØªØ±ÛŒÙ†\", \"ØªÙˆ\", \"Ú†Ù†Ø¯\", \"Ú†Ù†Ø¯ÛŒÙ†\", \"Ú†Ù‡\", \"Ø®ÛŒÙ„ÛŒ\", \"Ø¯ÛŒÚ¯Ø±\", \"Ø¯ÛŒÚ¯Ù‡\",\n",
    "              \"Ø·Ø¨ÛŒØ¹ØªØ§\", \"Ø¹Ù…Ø¯Ø§\", \"Ú¯Ø§Ù‡ÛŒ\", \"Ù†ÛŒØ²\", \"Ø§Ùˆ\", \"ØªÙˆ\", \"Ù…Ù†\", \"Ù…Ø§\", \"Ø·ÙˆØ±\", \"Ø¯Ùˆ\", \"Ù‡Ø±\", \"Ù‡Ù…Ù‡\", \"Ø§ÛŒÙ…\", \"Ø§Ù†Ø¯\", \"Ø§ÛŒØ¯\",\n",
    "              \"Ø§Ø³Øª\", \"Ù‡Ø³Øª\", \"Ø²Ø¯Ú¯ÛŒ\", \"Ø®ØµÙˆØµØ§\", \"Ù†\", \"Ø¨\", \"Ú©Ø³ÛŒ\", \"Ú†ÛŒØ²ÛŒ\", \"Ø¨Ø§Ù„Ø§Ø®Ø±Ù‡\", \"ÙˆÙ‚ØªÛŒ\", \"Ø²Ù…Ø§Ù†ÛŒ\", \"Ù…ÛŒ\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_word_data_20k = ['', 'Ù¾ÛŒØ§Ù…', 'Ø®ÙˆØ§Ù‡','Ø¯Ø§Ø±', 'Ø´Ø¯Ù‡', 'Ûµ', 'Ú¯Ø²Ø§Ø±', 'Ù‡Ø§', 'Ú¯ÛŒØ±', 'Ù‚Ø±Ø§Ø±', 'Ø§Ù†Øª', 'Û´', 'Ø­Ø¶ÙˆØ±', 'ÙˆÛŒ', 'Ù…ÛŒ',  'Ø§Ù†', 'Ø¨ÛŒÙ†', 'Ø³Ø§Ù„', 'Û·', 'Ú¯ÙØª', 'Ú©Ø§Ø±', 'ÛŒ', 'Ú©Ø±Ø¯Ù‡', 'Ø§Ù…Ø§', 'Ø§Ø¯Ø§Ù…Ù‡', 'Ø´Ùˆ', 'Ø±ÙˆØ²', 'Ú©Ù†', 'Ú¯Ø°Ø´ØªÙ‡', 'Ø¯Ø§Ø´Øª', 'Ù‡Ø§ÛŒ', 'Ø¨Ø§Ø²', 'Ø§Ù†Ø¬Ø§Ù…', 'Ø¯Ø§Ø¯', 'Û³', 'Ø¨Ø§ÛŒØ¯', 'Ø§ÛŒÙ†Ú©Ù‡', 'Ù¾ÛŒØ´', 'Ø¨ÛŒØ´', 'Ø¯Ùˆ', 'Ø¨Ø§Ø´', 'ØªÙˆØ§Ù†', 'Ø¯ÛŒÚ¯Ø±', 'Û²', 'Ø¹Ù†Ùˆ',  'Ø§Ù…Ø±ÙˆØ²', 'Û¹', 'Û¸', 'Ù¾Ø§ÛŒ', 'Ù…ÙˆØ±Ø¯', 'ØµÙˆØ±Øª', 'Ù…Ø±Ø¯Ù…', 'Ø¨', 'Ù‡Ø±', 'Ù…Ø§', 'Ø§ÛŒØ±', 'Ø±Ùˆ', 'Ø§Ø¸Ù‡Ø§Ø±', 'Û±', 'Ø§ÙØ²ÙˆØ¯', 'â€“', 'Û¶', 'Ø§', 'Ø±', 'Ø«',  'Û°', 'Ø´', 'Ø¯', 'Ú˜', 'Ú©', 'Ø¹', 'Ù‡', 'Ø¬', 'Ø²', 'Ù…', 'Øª', 'Ù¢', 'Ù£', 'Ù§', 'Ù‚', 'Ù„', 'Ù†', '@', ',', '\\u2066', 'Ù©', 'Ù¤', 'Ù¥', 'Ù¨', 'Ù¡', 'Ù¦', 'Ø³', 'Øµ', 'Ø¡', 'Ù', \"'\", 'Ã¶', 'Ú¯', 'ã€‹', 'Ø·', 'Ù¾', 'â€¦', 'Ø®', 'Ãª', 'Ø­', '\\u2067', '\\u2069', 'Û—', 'â€¢', 'Ùª', '\\u200b', 'Ã—', 'Ûš', 'Û–', 'Ú†', '\"', '\\u2063', 'Ø°', '\\uf0d8', 'Û”', 'Ã¼', 'ï»­', 'Øº', 'ï¸', 'ï·²', '\\u202c', '\\u202a', 'ï»«', 'Û™', 'Ø¶', 'Â·', 'Â¬', ';', 'Ø®Ø¨Ø± Ú¯Ø²Ø§Ø±ÛŒ','Ú¯Ø²Ø§Ø²Ø´','Ø®Ø¨Ø±Ù†Ú¯Ø§Ø±',]\n",
    "# # stop_word_data_17k = ['', 'Ø¯Ø±ØµØ¯', 'ÙˆÛŒ', 'Ú©Ù†', 'ØªÙˆØ§Ù†', 'Ù¾ÛŒØ§Ù…', 'Ù…Ø§', 'Ú¯ÙØª', 'Ø¯ÙˆÙ„Øª', 'Ø®ÙˆØ§Ù‡', 'Ø¨Ø§ÛŒØ¯', 'Ø§ÛŒÙ†Ú©Ù‡','Ø§Ù†', 'ØªÙˆÙ„ÛŒØ¯', 'Ø¯Ø§Ø±', 'Ø§Ú¯Ø±', 'Ù‡Ø³ØªÙ†Ø¯', , 'Ø´Ùˆ', 'Ù¾ÛŒØ´', 'ÙˆØ¬ÙˆØ¯','Ø§ÛŒØ±', 'Ø´Ø¯Ù‡', 'Ú©Ø§Ø±', 'Ø¨Ø±Ø®ÛŒ', 'Ø±Ùˆ', 'Ø¨ÛŒÙ†', 'ØªÙˆØ³Ø·', 'Ú¯ÛŒØ±', 'Ù†Ø¸Ø±', 'Ø¨Ø§Ø´', 'Ù‡Ù…Ú†Ù†ÛŒÙ†', 'Ø³Ø§Ù„', 'Ø¯Ùˆ', 'Ø¯Ø§Ø´Øª', 'Ø§Ù†Ø¬Ø§Ù…', 'Ø§ÙØ²Ø§ÛŒ', 'Ø§Ù…Ø§', 'Ù‡Ø±', 'Ø­Ø§Ù„', 'Ù‚Ø±Ø§Ø±', 'Ù…ÙˆØ±Ø¯', 'Ø§ÙØ²ÙˆØ¯', 'Ú¯Ø±ÙØªÙ‡',  'Ø§Ù†Ù‡Ø§', 'Ø§Ù†Øª', 'Ø¯Ù„ÛŒÙ„', 'Ø¨', 'Ø§Ø¯Ø§Ù…Ù‡', 'Ù…ÛŒ', 'Ø´Ø±Ú©Øª', 'Ø§Ø³Ø§Ø³', 'Ù¾Ø§ÛŒ', 'Ú¯Ø²Ø§Ø±', 'Ú©Ø±Ø¯Ù‡', 'Ø³Ø§Ø²', 'Ø¨Ø®Ø´', 'Ø¨ÛŒØ´', 'Ø¯Ø§Ø¯', 'Ù…ÙˆØ¶ÙˆØ¹', 'Ø±ÙˆØ²', 'ÛŒØ§', 'Ø´Ø±Ø§ÛŒØ·', 'Ø¨ÙˆØ¯Ù‡',  'Ù‡Ù…ÛŒÙ†', 'Ú©Ø§Ù‡Ø´', 'Ú¯Ø°Ø´ØªÙ‡', 'ØªÙˆØ¬Ù‡', 'ÛŒÚ©ÛŒ', 'Ø§Ù…Ø±ÙˆØ²', 'Ù…Ø§Ù‡', 'Ø²Ø§Ø±', 'Ø§Ù‚Ø¯Ø§Ù…', 'Ûµ', 'Ø¯Ø§Ø´ØªÙ‡', 'Û²', 'Ø¹Ù†Ùˆ', 'Ø¯Ø§Ø¯Ù‡', 'Ù‡Ø§ÛŒ', 'Ø¯ÛŒÚ¯Ø±', 'Ù†Ø³Ø¨Øª', 'Ø¨ÛŒØ§Ù†', 'Ù‡Ù…Ù‡', 'Û¸', 'Ø§Ø¹Ù„Ø§Ù…',  'Ù†Ù‚Ù„', 'Ù¾Ø³', 'Û·', 'Ù…ÛŒÙ„ÛŒÙˆÙ†', 'Û¹', 'Ø±', 'Û¶', 'Û´', 'Û³', 'Ù‡', 'Û°', 'Ú¯Ø±ÙˆÙ‡', 'Ø§Ø´Ø§Ø±Ù‡', 'Û±Û³Û¹Û¹', 'Û±', 'Ø¬', 'Ù…Ø±Ø¯Ù…', 'Ø§Ø³ØªÙØ§Ø¯Ù‡', 'ÛŒ', 'Øª', 'Ù¥', 'Ù¢', 'Ù¡', 'Ù£', 'Ù¦', 'Ø¯', 'â€“', 'Ú˜', 'Øµ', 'Ø¹', 'Ù…', 'Ù¾', 'Ø§', 'Ù¤', 'Ø´', 'Ù§', 'Ù©', 'Ú©', 'Ø²', 'Ù', 'Ø­', 'â€™', 'Ù†', ',', 'Ù¨', \"'\", 'Ú†', 'Ù ', 'â€¢', 'Â·', '\\u200b', 'Øº', 'â€¦', 'Ù„', 'Ø®', 'Ø°', 'Ø«', '\\uf0d8', 'Ø³', '@', 'Ù‚', 'Ø¡', 'Ø·', 'Ú¯', ';', 'Ù¬', 'Û—', '\\uf0fc', 'â€”', 'Ã±', 'Ã¡', 'ï»«', 'Ã—', 'âˆ’', 'Â±', '\"', 'ï´¾', 'Â°', 'ï»­',  'ðŸ”¹', 'Ûš', 'Û–', '\\u202b', 'ÅŸ', 'Ã©', 'Ø©', 'Û™']\n",
    "# stop_word_data_11k = ['', 'Ù¾ÛŒØ§Ù…', 'Ø§ÛŒÙ†Ú©Ù‡', 'Ø§ÛŒØ±', 'Ø´Ø¯Ù‡', 'Ú©Ù†', 'Ø¨Ø§Ø´', 'Ú©Ø´ÙˆØ±', 'Ù‡Ø§ÛŒ', 'Ú©Ø§Ø±', 'Ø±ÙˆØ²', 'Ø§Ø¹Ù„Ø§Ù…', 'Ú¯Ø²Ø§Ø±', 'Ù‡Ù…Ú†Ù†ÛŒÙ†', 'Ø³Ø§Ù„', 'Ú©Ø±Ø¯Ù‡', 'Ø¯Ø§Ø¯', 'Ø§Ù†Øª', 'Ø¹Ù†Ùˆ', 'Û³', 'Ø­Ø¶ÙˆØ±', 'Ø®ÙˆØ§Ù‡', 'ÙˆÛŒ', 'Ø§ÙØ²ÙˆØ¯', 'Ú¯ÙØª', 'Ø¨ÛŒØ§Ù†', 'ØªÙˆØ§Ù†', 'Ø´Ùˆ', 'Ø¨Ø®Ø´', 'Ø¨Ø§ÛŒØ¯', 'Ø¯Ø§Ø±', 'Ø¯Ø§Ø´Øª', 'Ø§Ù†', 'ØªÙˆØ¬Ù‡', 'Ù…ÙˆØ±Ø¯', 'Ø¯Ùˆ', 'Ø§Ù†Ø¬Ø§Ù…', 'Û²', 'Ø±Ùˆ', 'Ù‡Ø±', 'ÛŒØ§', 'ØµÙˆØ±Øª', 'Ù‡Ø³ØªÙ†Ø¯', 'Ø§Ù…Ø§', 'ÛŒ', 'Ù¢', 'Ú¯Ø°Ø´ØªÙ‡', 'Û´', 'Ù‚Ø±Ø§Ø±', 'Ø¨ÛŒØ´', 'Ø¯ÛŒÚ¯Ø±', 'Ø§Ø¯Ø§Ù…Ù‡', 'Ø§Ù…Ø±ÙˆØ²', 'Û°', 'Û¶', 'Ûµ', 'Û¹', 'Û·', 'Û¸', 'Û±', 'Ø§', 'Ù£', 'Ø¨', 'Ø³', 'â€“', 'Ù¨', 'Ø¬', 'Ù§', 'Ù¾', \"'\", 'Ø²', 'Ø±', 'Ù‡', 'Ø¹', 'Ø¡', 'Ù…', '\\u202c', 'Ø´', 'Ù¦', 'Ù†', 'Ù©', '@', 'Ûš', 'Ù¤', 'Ø¯', 'Øµ', 'Ú©', '\\u202b', 'Ø«', 'Ú†', 'Ù„', 'Ø·', 'Ø­', 'Ù', 'Øª', 'â€¦', 'Ù¥', '\\u2067', 'Ù¡', 'Ø°', 'Ù‚', 'â€”', 'Ù ', ',', 'Øº', '\\ufeff', 'Ùª', '\"', '\\u2069', 'â€¢', 'Û—', 'â€™', 'Ú˜', 'Ø®', 'Ð¸', 'Ðž', 'Ñ', 'Ð±', 'Ð¯', 'Ð¡', 'Ð²', 'Û”', 'Â·','Ø¨Ø±','ØªØ§','Ø¯Ø±','Ø¨Ù‡','Ø§Ø²', 'Ú¯', 'Ã–', 'Ã©', ';', '\\u206b', 'ã€‹', 'â €', '\\u202a', 'Û–', 'â‡', 'ï»«'] + stop_words1\n",
    "# stop_words = set(stop_word_data_20k + stop_word_data_11k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regex  patterns, prefixe list and postfix list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "raw_postfix = [\n",
    "        ('[\\u200c](ØªØ±(ÛŒÙ†?)?|Ú¯Ø±ÛŒ?|Ù‡Ø§ÛŒ?)' , ''),\n",
    "        (r'(ØªØ±(ÛŒÙ†?)?|Ú¯Ø±ÛŒ?)(?=$)' , ''), #ØªØ± ØŒ ØªØ±ÛŒÙ†ØŒ Ú¯Ø±ØŒ Ú¯Ø±ÛŒ Ø¯Ø± Ø¢Ø®Ø± Ú©Ù„Ù…Ù‡\n",
    "        (r'(?<=[^ Ø§Ùˆ])(Ù…|Øª|Ø´|Ù…Ø§Ù†|ØªØ§Ù†|Ø´Ø§Ù†)$' , ''), # Ø­Ø°Ù Ø´Ù†Ø§Ø³Ù‡ Ù‡Ø§ÛŒ Ù…Ø§Ù„Ú©ÛŒØª Ùˆ ÙØ¹Ù„ Ø¯Ø± Ø¢Ø®Ø± Ú©Ù„Ù…Ù‡\n",
    "        (r'(Ø§Ù†|Ø§Øª|Ú¯ÛŒ|Ú¯Ø±ÛŒ|Ù‡Ø§ÛŒ)$' , ''), #Ø§Ù†ØŒ Ø§ØªØŒ Ù‡Ø§ØŒ Ù‡Ø§ÛŒ Ø¢Ø®Ø± Ú©Ù„Ù…Ù‡  \n",
    "]\n",
    "\n",
    "raw_arabic_notation = [\n",
    "    # remove FATHATAN, DAMMATAN, KASRATAN, FATHA, DAMMA, KASRA, SHADDA, SUKUN\n",
    "    ('[\\u064B\\u064C\\u064D\\u064E\\u064F\\u0650\\u0651\\u0652]', ''),\n",
    "    ]\n",
    "\n",
    "raw_long_letters = [\n",
    "    (r' +', ' '),  # remove extra spaces\n",
    "    (r'\\n\\n+', '\\n'),  # remove extra newlines\n",
    "    (r'[Ù€\\r]', ''),  # remove keshide, carriage returns\n",
    "    ]\n",
    "\n",
    "raw_half_space = [\n",
    "    ('[\\u200c]', ''),\n",
    "]\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## verbs, mokassar,morakab list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "present_roots =['ØªÙˆØ§Ù†','Ø¨Ø§Ø´','Ø±Ùˆ','Ø¨Ø±','ÛŒØ§ÙˆØ±', 'ÛŒØ§Ù†Ø¯Ø§Ø²', 'ÛŒØ§ÛŒ','ÛŒØ§Ù†Ø¯ÛŒØ´','Ø¨Ø®Ø´','Ø¨Ø§Ø²','Ø®Ø±','Ø¨ÛŒÙ†','Ø´Ù†Ùˆ','Ø¯Ø§Ø±','Ø¯Ø§Ù†','Ø±Ø³Ø§Ù†','Ø´Ù†Ø§Ø³','Ú¯Ùˆ','Ú¯Ø°Ø§Ø±','ÛŒØ§Ø¨','Ù„Ø±Ø²','Ø³Ø§Ø²','Ø´Ùˆ','Ù†ÙˆÛŒØ³','Ø®ÙˆØ§Ù†','Ú©Ø§Ù‡','Ú¯ÛŒØ±','Ø®ÙˆØ§Ù‡','Ú©Ù†' ]\n",
    "\n",
    "past_roots = ['ØªÙˆØ§Ù†Ø³Øª','Ø¨ÙˆØ¯','Ú©Ø±Ø¯','Ø§ÙˆØ±Ø¯','Ø§Ù†Ø¯Ø§Ø®Øª','Ø§Ù…Ø¯','Ø®Ø±ÛŒØ¯','Ø¨Ø§Ø®Øª','Ø¨Ø±Ø¯','Ø±ÙØª','Ø§Ù†Ø¯ÛŒØ´ÛŒØ¯','Ø¨Ø®Ø´ÛŒØ¯','Ø¯ÛŒØ¯','Ø´Ù†ÛŒØ¯','Ø¯Ø§Ø´Øª','Ø¯Ø§Ù†Ø³Øª','Ø±Ø³Ø§Ù†Ø¯','Ø´Ù†Ø§Ø®Øª','Ú¯ÙØª','Ú¯Ø°Ø´Øª','ÛŒØ§ÙØª','Ù„Ø±Ø²ÛŒØ¯','Ø³Ø§Ø®Øª','Ø´Ø¯','Ù†ÙˆØ´Øª','Ø®ÙˆØ§Ù†Ø¯','Ú©Ø§Ø³Øª','Ú¯Ø±ÙØª','Ø®ÙˆØ§Ø³Øª']\n",
    "all_verbs_roots = present_roots + past_roots\n",
    "empty_list = ['','','','','','','']\n",
    "verb_prefix = ['Ù†Ù…ÛŒâ€Œ', 'Ù…ÛŒâ€Œ','Ù†','Ø¨',\"\" ]\n",
    "present_verb_postfix = ['Ù…','ÛŒ','Ø¯','ÛŒØ¯','Ù†Ø¯','ÛŒÙ…']\n",
    "past_verb_postfix = ['Ø§ÛŒÙ…','Ø§ÛŒØ¯','Ø§ÛŒ','Ø§Ù…','Ø§Ù†Ø¯']\n",
    "past_verb_postfix2 = ['Ù…','ÛŒ','ÛŒØ¯','Ù†Ø¯','ÛŒÙ…']\n",
    "present_verbs = []\n",
    "past_verbs = []\n",
    "all_verbs = {}\n",
    "for pref in verb_prefix:\n",
    "    for present_root, past_root in zip(present_roots, past_roots):\n",
    "        for post in past_verb_postfix2:\n",
    "            all_verbs[pref + past_root+post] = past_root\n",
    "        for post in past_verb_postfix:\n",
    "            all_verbs[past_root+'Ù‡â€Œ'+post] = past_root\n",
    "        for post in present_verb_postfix:\n",
    "            all_verbs[pref+present_root+post] = present_root\n",
    "            \n",
    "words_list = ['Ø§Ø¯Ø¨', 'Ø¢Ø¯Ø§Ø¨', 'Ø·Ø±Ù', 'Ø§Ø·Ø±Ø§Ù', 'Ø­Ù‚ÛŒÙ‚Øª', 'Ø­Ù‚Ø§ÛŒÙ‚', 'Ù…ÙˆØ¬', 'Ø§Ù…ÙˆØ§Ø¬', 'Ø§Ø¯ÛŒØ¨', 'Ø§Ø¯Ø¨Ø§', 'Ø¹Ù…Ù‚', 'Ø§Ø¹Ù…Ø§Ù‚', 'Ø®Ø²ÛŒÙ†Ù‡', 'Ø®Ø²Ø§Ø¦Ù†', 'Ù…Ø±Ú©Ø²', 'Ù…Ø±Ø§Ú©Ø²', 'Ø§Ø«Ø±', 'Ø¢Ø«Ø§Ø±', 'Ø¹Ø§Ù„Ù…', 'Ø¹Ù„Ù…Ø§', 'Ø®Ø¨Ø±', 'Ø§Ø®Ø¨Ø§Ø±', 'Ù…ÙˆÙ‚Ø¹', 'Ù…ÙˆØ§Ù‚Ø¹', 'Ø§Ø³ÛŒØ±', 'Ø§Ø³Ø±Ø§', 'Ø¹Ù„Ù…', 'Ø¹Ù„ÙˆÙ…', 'Ø¯ÙˆØ±Ù‡', 'Ø§Ø¯ÙˆØ§Ø±', 'Ù…ØµØ±Ù', 'Ù…ØµØ§Ø±Ù', 'Ø§Ø³Ù…', 'Ø§Ø³Ø§Ù…ÛŒ', 'Ø¹Ù„Ø§Ù…Øª', 'Ø¹Ù„Ø§Ø¦Ù…', 'Ø¯ÛŒÙ†', 'Ø§Ø¯ÛŒØ§Ù†', 'Ù…Ø¹Ø±ÙØª', 'Ù…Ø¹Ø§Ø±Ù', 'Ø§Ø³Ø·ÙˆØ±Ù‡', 'Ø§Ø³Ø§Ø·ÛŒØ±', 'Ø¹Ù„Øª', 'Ø¹Ù„Ù„', 'Ø¯ÙØªØ±', 'Ø¯ÙØ§ØªØ±', 'Ù…Ø¨Ø­Ø«', 'Ù…Ø¨Ø§Ø­Ø«', 'Ø§Ù…ÛŒØ±', 'Ø§Ù…Ø±Ø§', 'Ø¹Ù‚ÛŒØ¯Ù‡', 'Ø¹Ù‚Ø§Ø¦Ø¯', 'Ø°Ø®ÛŒØ±Ù‡', 'Ø°Ø®Ø§ÛŒØ±', 'Ù…Ø§Ø¯Ù‡', 'Ù…ÙˆØ§Ø¯', 'Ø§Ù…Ø±', 'Ø§ÙˆØ§Ù…Ø±', 'Ø¹Ù…Ù„', 'Ø§Ø¹Ù…Ø§Ù„', 'Ø±ÙÛŒÙ‚', 'Ø±ÙÙ‚Ø§', 'Ù…Ø°Ù‡Ø¨', 'Ù…Ø°Ø§Ù‡Ø¨', 'Ø§Ù…Ø§Ù…', 'Ø§Ø¦Ù…Ù‡', 'Ø¹ÛŒØ¯', 'Ø§Ø¹ÛŒØ§Ø¯', 'Ø±Ø§ÛŒ', 'Ø¢Ø±Ø§', 'Ù…ØµÛŒØ¨Øª', 'Ù…ØµØ§Ø¦Ø¨', 'Ø§ØµÙ„', 'Ø§ØµÙˆÙ„', 'Ø¹Ù†ØµØ±', 'Ø¹Ù†Ø§ØµØ±', 'Ø±Ø³Ù…', 'Ø±Ø³ÙˆÙ…', 'Ù…Ø¹Ø¨Ø¯', 'Ù…Ø¹Ø§Ø¨Ø¯', 'Ø§ÙÙ‚', 'Ø¢ÙØ§Ù‚', 'Ø¹Ø§Ø·ÙÙ‡', 'Ø¹ÙˆØ§Ø·Ù', 'Ø±Ø§Ø¨Ø·Ù‡', 'Ø±ÙˆØ§Ø¨Ø·', 'Ù…Ø³Ø¬Ø¯', 'Ù…Ø³Ø§Ø¬Ø¯', 'Ø¨ÛŒØª', 'Ø§Ø¨ÛŒØ§Øª', 'Ø¹Ø¶Ùˆ', 'Ø§Ø¹Ø¶Ø§', 'Ø±Ù…Ø²', 'Ø±Ù…ÙˆØ²', 'Ù…Ø¹Ø¨Ø±', 'Ù…Ø¹Ø§Ø¨Ø±', 'ØªØ§Ø¬Ø±', 'ØªØ¬Ø§Ø±', 'Ø¹Ø¨Ø§Ø±Øª', 'Ø¹Ø¨Ø§Ø±Ø§Øª', 'Ø±Ø¬Ù„', 'Ø±Ø¬Ø§Ù„', 'Ù…Ø¸Ù‡Ø±', 'Ù…Ø¸Ø§Ù‡Ø±', 'ØªØµÙˆÛŒØ±', 'ØªØµØ§ÙˆÛŒØ±', 'Ø¹Ø¬ÛŒØ¨', 'Ø¹Ø¬Ø§ÛŒØ¨', 'Ø±Ù‚Ù…', 'Ø§Ø±Ù‚Ø§Ù…', 'Ù…Ù†Ø¸Ø±Ù‡', 'Ù…Ù†Ø§Ø¸Ø±', 'Ø¬Ø¯', 'Ø§Ø¬Ø¯Ø§Ø¯', 'ÙÙ‚ÛŒÙ‡', 'ÙÙ‚Ù‡Ø§', 'Ø²Ø§ÙˆÛŒÙ‡', 'Ø²ÙˆØ§ÛŒØ§', 'Ù…Ø±Ø¶', 'Ø§Ù…Ø±Ø§Ø¶', 'Ø¬Ø§Ù†Ø¨', 'Ø¬ÙˆØ§Ù†Ø¨', 'ÙÙ†', 'ÙÙ†ÙˆÙ†', 'Ø²Ø¹ÛŒÙ…', 'Ø²Ø¹Ù…Ø§', 'Ù…ÙˆØ±Ø¯', 'Ù…ÙˆØ§Ø±Ø¯', 'Ø¬Ø²ÛŒØ±Ù‡', 'Ø¬Ø²Ø§ÛŒØ±', 'ÙÚ©Ø±', 'Ø§ÙÚ©Ø§Ø±', 'Ø³Ø§Ù†Ø­Ù‡', 'Ø³ÙˆØ§Ù†Ø­', 'Ù…Ø±Ø­Ù„Ù‡', 'Ù…Ø±Ø§Ø­Ù„', 'Ø¬Ø¨Ù„', 'Ø¬Ø¨Ø§Ù„', 'ÙØ±ÛŒØ¶Ù‡', 'ÙØ±Ø§ÛŒØ¶', 'Ø³Ù„Ø·Ø§Ù†', 'Ø³Ù„Ø§Ø·ÛŒÙ†', 'Ù…ÙÙ‡ÙˆÙ…', 'Ù…ÙØ§Ù‡ÛŒÙ…', 'Ø¬Ø±ÛŒÙ…Ù‡', 'Ø¬Ø±Ø§ÛŒÙ…', 'ÙØ¹Ù„', 'Ø§ÙØ¹Ø§Ù„', 'Ø´Ø¹Ø±', 'Ø§Ø´Ø¹Ø§Ø±', 'Ù…Ù†Ø¨Ø¹', 'Ù…Ù†Ø§Ø¨Ø¹', 'Ø­Ø§Ø¯Ø«Ù‡', 'Ø­ÙˆØ§Ø¯Ø«', 'ÙÙ‚ÛŒØ±', 'ÙÙ‚Ø±Ø§', 'Ø´Ø§Ø¹Ø±', 'Ø´Ø¹Ø±Ø§', 'Ù…Ú©Ø§Ù†', 'Ø§Ù…Ø§Ú©Ù†', 'Ø­Ø´Ù…', 'Ø§Ø­Ø´Ø§Ù…', 'Ù‚Ø§Ø¹Ø¯Ù‡', 'Ù‚ÙˆØ§Ø¹Ø¯']\n",
    "mokassar_dict ={}\n",
    "for i in range(0,len(words_list),2):\n",
    "    mokassar_dict[words_list[i+1]] = words_list[i]\n",
    "                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(txt, seps):\n",
    "    default_sep = seps[0]    \n",
    "    # we skip seps[0] because that's the default separator\n",
    "    for sep in seps[1:]:\n",
    "        txt = txt.replace(sep, default_sep)\n",
    "    return [i.strip() for i in txt.split(default_sep)]\n",
    "\n",
    "#get doc and return (word,doc id)\n",
    "def tokenizer(text):\n",
    "    for l in english_char:\n",
    "        text = text.replace(l,\"\")\n",
    "    word_list = split(text.strip().replace(\"\\n\",\" \"), sep_list)\n",
    "    word_list = [x for x in word_list if not x.startswith(\"ir\") and not x.startswith(\"NID\")]\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation_marks(word_list):\n",
    "#     word_list = list(word_list)\n",
    "    for i in range(len(word_list)):\n",
    "        for mark in punctuation_marks:\n",
    "            word_list[i] = word_list[i].replace(mark,\"\")\n",
    "\n",
    "    return word_list\n",
    "\n",
    "\n",
    "def edit_long_letters(word_list):\n",
    "    patterns_compiled = [(re.compile(x[0]), x[1]) for x in raw_long_letters]\n",
    "    for pattern, rep in patterns_compiled:\n",
    "        for i in range(len(word_list)):\n",
    "            word_list[i] = pattern.sub(rep, word_list[i])\n",
    "    return word_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def remove_arabic_notation(word_list):\n",
    "    patterns_compiled = [(re.compile(x[0]), x[1]) for x in raw_arabic_notation]\n",
    "    for pattern, rep in patterns_compiled:\n",
    "        for i in range(len(word_list)):\n",
    "            word_list[i] = pattern.sub(rep, word_list[i])\n",
    "    return word_list\n",
    "\n",
    "\n",
    "def create_translation_table(src_list, dst_list):\n",
    "     return dict((ord(a), b) for a, b in zip(src_list, dst_list)) #map src unicode to dst char\n",
    "\n",
    "    \n",
    "def char_digit_Unification(word_list): \n",
    "    word_list = [x for x in word_list if not x.isnumeric() or (x.isnumeric() and float(x)<3000)]\n",
    "    translation_src, translation_dst = ' Ù‰ÙƒÙŠâ€œâ€', ' ÛŒÚ©ÛŒ\"\"'\n",
    "    translation_src += 'Ø¦0123456789Ø£Ø¥Ø¢%'\n",
    "    translation_dst += 'ÛŒÛ°Û±Û²Û³Û´ÛµÛ¶Û·Û¸Û¹Ø§Ø§Ø§Ùª'\n",
    "    translations_table = create_translation_table(translation_src, translation_dst)\n",
    "    for i in range(len(word_list)):\n",
    "        word_list[i] = word_list[i].translate(translations_table)\n",
    "    return word_list\n",
    "\n",
    "def remove_mokassar(word_list):\n",
    "    for i in range(len(word_list)):\n",
    "        word_list[i] = mokassar_dict.get(word_list[i],word_list[i])\n",
    "        \n",
    "    return word_list\n",
    "    \n",
    "\n",
    "\n",
    "def verb_Steaming(word_list):\n",
    "    for i in range(len(word_list)):\n",
    "        word_list[i] = all_verbs.get(word_list[i],word_list[i])\n",
    "        \n",
    "    return word_list\n",
    "    \n",
    "\n",
    "def remove_postfix(word_list):\n",
    "\n",
    "    patterns_compiled = [(re.compile(x[0]), x[1]) for x in raw_postfix]\n",
    "    for pattern, rep in patterns_compiled:\n",
    "        for i in range(len(word_list)):\n",
    "            if len(word_list[i])>4 and  not (word_list[i] in all_verbs_roots):\n",
    "                word_list[i] = pattern.sub(rep, word_list[i])\n",
    "    return word_list\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def remove_prefix(word_list):\n",
    "    starts =['Ø¨ÛŒâ€Œ' , 'Ù†Ø§', 'Ø¨Ø§']\n",
    "    for i in range(len(word_list)):\n",
    "        for start in starts:\n",
    "            if word_list[i].startswith(start) and len(word_list[i])>4:\n",
    "                word_list[i] = word_list[i][len(start):]\n",
    "                break\n",
    "\n",
    "    return word_list\n",
    "\n",
    "def morakab_Unification(word_list):\n",
    "    patterns_compiled = [(re.compile(x[0]), x[1]) for x in raw_half_space]\n",
    "    for pattern, rep in patterns_compiled:\n",
    "        for i in range(len(word_list)):\n",
    "            word_list[i] = pattern.sub(rep, word_list[i])\n",
    "    return word_list\n",
    "\n",
    "#all to gether \n",
    "def normalizer(word_list):\n",
    "    #remove punc\n",
    "    for i in range(len(word_list)):\n",
    "        for mark in punctuation_marks:\n",
    "            word_list[i] = word_list[i].replace(mark,\"\")\n",
    "            \n",
    "    #edit long letters\n",
    "    patterns_compiled = [(re.compile(x[0]), x[1]) for x in raw_long_letters]\n",
    "    for pattern, rep in patterns_compiled:\n",
    "        for i in range(len(word_list)):\n",
    "            word_list[i] = pattern.sub(rep, word_list[i])\n",
    "            \n",
    "    #remove mokassar\n",
    "    for i in range(len(word_list)):\n",
    "        word_list[i] = mokassar_dict.get(word_list[i],word_list[i])\n",
    "    \n",
    "    #remove_arabic_notation\n",
    "    patterns_compiled = [(re.compile(x[0]), x[1]) for x in raw_arabic_notation]\n",
    "    for pattern, rep in patterns_compiled:\n",
    "        for i in range(len(word_list)):\n",
    "            word_list[i] = pattern.sub(rep, word_list[i])\n",
    "    \n",
    "    #char_digit_Unification\n",
    "    word_list = [x for x in word_list if not x.isnumeric() or (x.isnumeric() and float(x)<3000)]\n",
    "    translation_src, translation_dst = ' Ù‰ÙƒÙŠâ€œâ€', ' ÛŒÚ©ÛŒ\"\"'\n",
    "    translation_src += 'Ø¦0123456789Ø£Ø¥Ø¢%'\n",
    "    translation_dst += 'ÛŒÛ°Û±Û²Û³Û´ÛµÛ¶Û·Û¸Û¹Ø§Ø§Ø§Ùª'\n",
    "    translations_table = create_translation_table(translation_src, translation_dst)\n",
    "    for i in range(len(word_list)):\n",
    "        word_list[i] = word_list[i].translate(translations_table)\n",
    "    \n",
    "    #verb_Steaming\n",
    "    for i in range(len(word_list)):\n",
    "        word_list[i] = all_verbs.get(word_list[i],word_list[i])\n",
    "        \n",
    "    #remove_prefix\n",
    "    starts =['Ø¨ÛŒâ€Œ' , 'Ù†Ø§', 'Ø¨Ø§']\n",
    "    for i in range(len(word_list)):\n",
    "        for start in starts:\n",
    "            if word_list[i].startswith(start) and len(word_list[i])>4:\n",
    "                word_list[i] = word_list[i][len(start):]\n",
    "                break\n",
    "    \n",
    "    #remove_postfix\n",
    "    patterns_compiled = [(re.compile(x[0]), x[1]) for x in raw_postfix]\n",
    "    for pattern, rep in patterns_compiled:\n",
    "        for i in range(len(word_list)):\n",
    "            if len(word_list[i])>4 and  not (word_list[i] in all_verbs_roots):\n",
    "                word_list[i] = pattern.sub(rep, word_list[i])\n",
    "    #morakab_Unification\n",
    "    patterns_compiled = [(re.compile(x[0]), x[1]) for x in raw_half_space]\n",
    "    for pattern, rep in patterns_compiled:\n",
    "        for i in range(len(word_list)):\n",
    "            word_list[i] = pattern.sub(rep, word_list[i])\n",
    "            \n",
    "    for word in word_list:\n",
    "        if word in stop_words:\n",
    "            word_list.remove(word)\n",
    "    \n",
    "    return [x for x in word_list if len(x) >= 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_data = pd.DataFrame()\n",
    "pd_list = []\n",
    "pd_list .append(pd.read_excel('IR00_3_11k News.xlsx', engine = 'openpyxl'))\n",
    "pd_list .append(pd.read_excel('IR00_3_17k News.xlsx', engine = 'openpyxl'))\n",
    "pd_list .append(pd.read_excel('IR00_3_20k News.xlsx', engine = 'openpyxl'))\n",
    "# pd_list .append(pd.read_excel('3.xlsx', engine = 'openpyxl'))\n",
    "# pd_list .append(pd.read_excel('2.xlsx', engine = 'openpyxl'))\n",
    "# pd_list .append(pd.read_excel('1.xlsx', engine = 'openpyxl'))\n",
    "for df in pd_list:\n",
    "    all_data = all_data.append(df,ignore_index=True)\n",
    "all_data[\"topic\"].replace({\"political\": \"politics\", \"sport\": \"sports\"}, inplace=True)\n",
    "all_data = all_data[all_data.content != 'None\\n\\n']\n",
    "all_data = all_data.reset_index(drop=True)\n",
    "all_data.index +=1\n",
    "all_data['id'] = all_data.index\n",
    "contents = all_data['content'].to_list()\n",
    "doc_ids = all_data['id'].to_list()\n",
    "doc_url = all_data['url']\n",
    "docs_topic = all_data['topic']\n",
    "docs_num = len(doc_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create inverted index and change docs content to bag of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dictionary(contents, doc_ids):\n",
    "    for content, id in zip(contents,doc_ids):\n",
    "        doc_tokens = tokenizer(content)\n",
    "        doc_normalized_tokens = normalizer(doc_tokens)\n",
    "        term_freq_dict = dict(collections.Counter(doc_normalized_tokens))\n",
    "        \n",
    "        for term, freq in term_freq_dict.items():\n",
    "            term_freq_dict[term] = (1 + math.log((freq), 10))\n",
    "        contents[id-1] = term_freq_dict\n",
    "        for term,freq in term_freq_dict.items():\n",
    "            term_postings = inverted_index.get(term,{})\n",
    "            term_postings[id] = freq\n",
    "            inverted_index[term] = term_postings\n",
    "\n",
    "def calculate_tf_idf_weight(contents):\n",
    "    docs_len = []\n",
    "    for doc in contents:\n",
    "        length = 0\n",
    "        for term in doc.keys():\n",
    "            df = len(inverted_index[term])\n",
    "            idf = math.log(docs_num/df,10)\n",
    "            doc[term] *= idf \n",
    "            length += doc[term]\n",
    "        docs_len.append( math.sqrt(length))\n",
    "    return docs_len\n",
    "\n",
    "            \n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "inverted_index ={}\n",
    "create_dictionary(contents, doc_ids)\n",
    "docs_len = calculate_tf_idf_weight(contents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save inverted_index and docs_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "file_inverted_index = open('ph3_inverted_index.obj', 'wb') \n",
    "pickle.dump(inverted_index, file_inverted_index)\n",
    "file_inverted_index.close()\n",
    "inverted_index = {}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_docs_len = open('ph3_docs_len.obj', 'wb') \n",
    "pickle.dump(docs_len, file_docs_len)\n",
    "file_docs_len.close()\n",
    "docs_len = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_contents = open('ph3_contents.obj', 'wb') \n",
    "pickle.dump(contents, file_contents)\n",
    "file_contents.close()\n",
    "contents =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inverted_index['Ø³Ù„Ø§Ù…']\n",
    "# docs_len =[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read inverted_index and docs_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "readed = open('ph3_inverted_index.obj', 'rb') \n",
    "inverted_index = pickle.load(readed)\n",
    "readed = open('docs_len.obj', 'rb') \n",
    "docs_len = pickle.load(readed)\n",
    "readed = open('ph3_contents.obj', 'rb') \n",
    "contents = pickle.load(readed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans Clustring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_random_centroids(k):\n",
    "    centroid_docs_id = (np.random.uniform(0, docs_num, size = k).astype(int))\n",
    "    centroids = { x:contents[x-1] for x in centroid_docs_id}\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_to_leader(centeroids,centroids_len, contents, first):\n",
    "    \n",
    "    clusters = {x:[] for x in centeroids.keys()}\n",
    "    for doc_id in doc_ids:\n",
    "        relevent_flag = False\n",
    "        doc_content = contents[doc_id-1]\n",
    "        max_Score = 0\n",
    "        best_centroid = 0\n",
    "        for centroid_id, centroid_content in centeroids.items():\n",
    "            centroid_score = 0\n",
    "            for term in doc_content.keys():\n",
    "                centroid_score += (doc_content[term] * centroid_content.get(term, 0))\n",
    "            \n",
    "            if first == 0:\n",
    "                centroid_score /= docs_len[centroid_id-1]\n",
    "            else: \n",
    "                centroid_score /= centroids_len[centroid_id]\n",
    "            \n",
    "            if centroid_score> max_Score:\n",
    "                max_Score = centroid_score\n",
    "                best_centroid = centroid_id\n",
    "                relevent_flag = True\n",
    "        if relevent_flag:\n",
    "            clusters[best_centroid].append(doc_id)\n",
    "    return clusters\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_cluster(clusters, k):\n",
    "    new_centroids = {-1*x:{} for x in range(1,k+1)}\n",
    "    centroids_len = {-1*x:0 for x in range(1,k+1)}\n",
    "    counter = -1\n",
    "    for cluster_followers in clusters.values():\n",
    "        new_centroid = {}\n",
    "        cluster_sum = 0\n",
    "        for doc_id in cluster_followers:\n",
    "            for term,w_t in contents[doc_id-1].items():\n",
    "                w = new_centroid.get(term,0)\n",
    "                new_centroid[term] = w + w_t\n",
    "                \n",
    "        for key in new_centroid.keys():\n",
    "            new_centroid[key] /= len(cluster_followers)\n",
    "            cluster_sum += new_centroid[key]**2\n",
    "        new_centroids[counter] = new_centroid\n",
    "        centroids_len[counter] =math.sqrt(cluster_sum)\n",
    "        counter -= 1\n",
    "    return  new_centroids, centroids_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kmeans_clustering(k, number_of_iteration):\n",
    "    new_centroids = choose_random_centroids(k)\n",
    "    clusters = []\n",
    "    centroids_len = {}\n",
    "    for i in tqdm(range(number_of_iteration)):\n",
    "        clusters =  assign_to_leader(new_centroids,centroids_len ,contents, i)\n",
    "\n",
    "        if i != number_of_iteration-1: \n",
    "            new_centroids, centroids_len = avg_cluster(clusters, k)\n",
    "\n",
    "    return clusters, new_centroids, centroids_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# clusters, centroids, centroids_len = Kmeans_clustering(10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [19:52<00:00, 238.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19min 52s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clusters, centroids, centroids_len = Kmeans_clustering(100,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673\n",
      "765\n",
      "662\n",
      "564\n",
      "580\n",
      "90\n",
      "869\n",
      "416\n",
      "141\n",
      "1254\n",
      "506\n",
      "576\n",
      "187\n",
      "262\n",
      "819\n",
      "240\n",
      "1031\n",
      "462\n",
      "241\n",
      "484\n",
      "598\n",
      "157\n",
      "1156\n",
      "482\n",
      "258\n",
      "308\n",
      "247\n",
      "384\n",
      "853\n",
      "445\n",
      "136\n",
      "617\n",
      "645\n",
      "322\n",
      "304\n",
      "565\n",
      "824\n",
      "364\n",
      "528\n",
      "745\n",
      "680\n",
      "150\n",
      "482\n",
      "613\n",
      "869\n",
      "1194\n",
      "146\n",
      "780\n",
      "500\n",
      "391\n",
      "78\n",
      "138\n",
      "787\n",
      "526\n",
      "445\n",
      "601\n",
      "529\n",
      "406\n",
      "357\n",
      "795\n",
      "181\n",
      "784\n",
      "163\n",
      "264\n",
      "169\n",
      "693\n",
      "590\n",
      "597\n",
      "242\n",
      "717\n",
      "397\n",
      "271\n",
      "300\n",
      "291\n",
      "92\n",
      "519\n",
      "199\n",
      "230\n",
      "152\n",
      "543\n",
      "783\n",
      "279\n",
      "591\n",
      "567\n",
      "616\n",
      "504\n",
      "547\n",
      "504\n",
      "629\n",
      "356\n",
      "731\n",
      "586\n",
      "1047\n",
      "354\n",
      "485\n",
      "455\n",
      "585\n",
      "439\n",
      "521\n",
      "540\n"
     ]
    }
   ],
   "source": [
    "for key in clusters.keys():\n",
    "    print(len(clusters[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save clusters data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "file_clusters = open('clusters.obj', 'wb') \n",
    "pickle.dump(clusters, file_clusters)\n",
    "file_clusters.close()\n",
    "clusters = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_centroids = open('centroids.obj', 'wb') \n",
    "pickle.dump(centroids, file_centroids)\n",
    "file_centroids.close()\n",
    "centroids = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_centroids_len = open('centroids_len.obj', 'wb') \n",
    "pickle.dump(centroids_len, file_centroids_len)\n",
    "file_centroids_len.close()\n",
    "centroids_len = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read clusters data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "readed = open('clusters.obj', 'rb') \n",
    "clusters = pickle.load(readed)\n",
    "readed = open('centroids.obj', 'rb') \n",
    "centroids = pickle.load(readed)\n",
    "readed = open('centroids_len.obj', 'rb') \n",
    "centroids_len = pickle.load(readed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## query responding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_query(query_str):\n",
    "    query_tokens = tokenizer(query_str)\n",
    "    query_normalized_tokens = normalizer(query_tokens)\n",
    "    term_freq_dict = dict(collections.Counter(query_normalized_tokens))\n",
    "    query_terms = {}\n",
    "    for  term,freq in term_freq_dict.items():\n",
    "        if term in inverted_index.keys() and term not in stop_words:\n",
    "            query_terms[term] = 1+math.log(freq,10)\n",
    "    print(query_terms)\n",
    "    return query_terms\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_leader(centroids, centroids_len, query_terms, k,b):\n",
    "    similarity_score = {-1*x:0 for x in range(1,k+1)}\n",
    "    for term, w_q in query_terms.items():\n",
    "        for centroid_id, centroid_content in centroids.items():\n",
    "            similarity_score[centroid_id] += w_q * centroid_content.get(term, 0)\n",
    "    for key in similarity_score.keys():\n",
    "        similarity_score[key] /= centroids_len[key]\n",
    "    return sorted(similarity_score, key=similarity_score.get, reverse=True)[:b]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_docs(query_terms, cluster_docs):\n",
    "    docs_score = {x:0 for x in cluster_docs}\n",
    "    for term,Wt_q in query_terms.items():\n",
    "        for doc_id in cluster_docs:\n",
    "            docs_score[doc_id] += Wt_q * contents[doc_id-1].get(term, 0)\n",
    "    \n",
    "    for doc_id in cluster_docs:\n",
    "        docs_score[doc_id] /= docs_len[doc_id-1]\n",
    "    return docs_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_k(docs_similarity, k):\n",
    "\n",
    "    heap = [(-value, key) for key,value in docs_similarity.items()]\n",
    "    largest = heapq.nsmallest(k, heap)\n",
    "    top_k = [(y, -x) for x,y in largest]\n",
    "    return top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_responding(clusters_num, b):\n",
    "    query = input(\"enter query: \")\n",
    "    query_terms = vectorize_query(query)\n",
    "    best_leader_ids = find_best_leader(centroids, centroids_len, query_terms, clusters_num,b)\n",
    "    print(best_leader_ids)\n",
    "    cluster_docs = list(set(clusters[best_leader_ids[0]] + clusters[best_leader_ids[1]]))\n",
    "    docs_similarity = score_docs(query_terms, cluster_docs)\n",
    "    a = datetime.datetime.now()\n",
    "    top_docs = extract_top_k(docs_similarity, 50)\n",
    "    b = datetime.datetime.now()\n",
    "\n",
    "    print(\"{:<5} result in {} ms\\nid \\t(score)\\t\\t  -> link\\n\".format(len(top_docs),1000*(b-a).total_seconds()))\n",
    "    for doc_id, score in top_docs:\n",
    "        print( \"{:<5}({}) -> {} \".format(doc_id,\"-\", doc_url[doc_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter query: Ø§Ø¹Ø²Ø§Ù… Ú©Ø§Ø±ÙˆØ§Ù† Ù‡Ø§ÛŒ Ø§Ø±Ø¯ÙˆÛŒ Ø±Ø§Ù‡ÛŒØ§Ù† Ù†ÙˆØ±\n",
      "{'Ø§Ø¹Ø²Ø§Ù…': 1.0, 'Ú©Ø§Ø±Ùˆ': 1.0, 'Ù‡Ø§ÛŒ': 1.0, 'Ø§Ø±Ø¯ÙˆÛŒ': 1.0, 'Ø±Ø§Ù‡ÛŒ': 1.0, 'Ù†ÙˆØ±': 1.0}\n",
      "[-32, -41, -15, -33]\n",
      "50    result in 0.0 ms\n",
      "id \t(score)\t\t  -> link\n",
      "\n",
      "9702 (-) -> https://www.isna.ir/news/98021106170/Ø­Ø¶ÙˆØ±-Ù„Ø±Ø³ØªØ§Ù†ÛŒ-Ù‡Ø§-Ø¯Ø±-Ø¢ÛŒÛŒÙ†-Ø§ÙØªØªØ§Ø­ÛŒÙ‡-Ø§Ø±Ø¯ÙˆÙ‡Ø§ÛŒ-Ø±Ø§Ù‡ÛŒØ§Ù†-Ù†ÙˆØ±-ØºØ±Ø¨-Ú©Ø´ÙˆØ± \n",
      "9934 (-) -> https://www.isna.ir/news/98032712899/Ø­Ø¶ÙˆØ±-Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒØ§Ù†-Ú†Ù‡Ø§Ø±Ù…Ø­Ø§Ù„-Ùˆ-Ø¨Ø®ØªÛŒØ§Ø±ÛŒ-Ø¯Ø±-ÛŒØ§Ø¯Ù…Ø§Ù†-Ù‡Ø§ÛŒ-Ø¯ÙØ§Ø¹-Ù…Ù‚Ø¯Ø³ \n",
      "9004 (-) -> https://www.isna.ir/news/99092821569/Ø³Ø±Ø¯Ø§Ø±-Ú©Ø§Ø±Ú¯Ø±-Ù…Ø§Ù‡ÛŒØª-Ø±Ø§Ù‡ÛŒØ§Ù†-Ù†ÙˆØ±-Ù†Ø¨Ø§ÛŒØ¯-ØªØºÛŒÛŒØ±-Ú©Ù†Ø¯ \n",
      "10656(-) -> https://www.isna.ir/news/98080904715/Ù…Ø±Ø§Ø³Ù…-Ø§ÙØªØªØ§Ø­ÛŒÙ‡-Ø±Ø§Ù‡ÛŒØ§Ù†-Ù†ÙˆØ±-Ø¯Ø§Ù†Ø´-Ø¢Ù…ÙˆØ²ÛŒ-Ø¨Ø±Ú¯Ø²Ø§Ø±-Ù…ÛŒ-Ø´ÙˆØ¯ \n",
      "10031(-) -> https://www.isna.ir/news/98041910370/Ø®ÙˆØ²Ø³ØªØ§Ù†ÛŒ-Ù‡Ø§-Ø¨Ù‡-Ù…Ù†Ø§Ø·Ù‚-Ø¹Ù…Ù„ÛŒØ§ØªÛŒ-ØºØ±Ø¨-Ú©Ø´ÙˆØ±-Ù…ÛŒ-Ø±ÙˆÙ†Ø¯ \n",
      "10043(-) -> https://www.isna.ir/news/98041910370/Ø®ÙˆØ²Ø³ØªØ§Ù†ÛŒ-Ù‡Ø§-Ø¨Ù‡-Ù…Ù†Ø§Ø·Ù‚-Ø¹Ù…Ù„ÛŒØ§ØªÛŒ-ØºØ±Ø¨-Ú©Ø´ÙˆØ±-Ù…ÛŒ-Ø±ÙˆÙ†Ø¯ \n",
      "10692(-) -> https://www.isna.ir/news/98081408553/Ø§Ø¹Ù„Ø§Ù…-Ø¬Ø²Ø¦ÛŒØ§Øª-Ø­Ø¶ÙˆØ±-Ø¯Ø§Ù†Ø´-Ø¢Ù…ÙˆØ²Ø§Ù†-Ú¯ÛŒÙ„Ø§Ù†ÛŒ-Ø¯Ø±-Ø§Ø±Ø¯ÙˆÙ‡Ø§ÛŒ-Ø±Ø§Ù‡ÛŒØ§Ù†-Ù†ÙˆØ± \n",
      "10835(-) -> https://www.isna.ir/news/98091309743/ØªÙˆØ¶ÛŒØ­Ø§Øª-Ø³Ø±Ø¯Ø§Ø±-Ø¹Ø¨Ø¯Ø§Ù„Ù„Ù‡-Ù¾ÙˆØ±-Ø¯Ø±Ø¨Ø§Ø±Ù‡-Ø¨Ø²Ø±Ú¯ØªØ±ÛŒÙ†-Ú©Ø§Ø±ÙˆØ§Ù†-Ø±Ø§Ù‡ÛŒØ§Ù†-Ù†ÙˆØ±-Ú©Ø´ÙˆØ± \n",
      "10379(-) -> https://www.isna.ir/news/98062110107/Ù…ÛŒØ²Ø¨Ø§Ù†ÛŒ-Ø§Ø²-Û±Û°Û°-Ù‡Ø²Ø§Ø±-Ø²Ø§Ø¦Ø±-Ø±Ø§Ù‡ÛŒØ§Ù†-Ù†ÙˆØ±-Ø¯Ø±-Ø§Ø±Ø¯ÙˆÚ¯Ø§Ù‡-Ø´Ù‡ÛŒØ¯-Ø±Ø³ØªÚ¯Ø§Ø± \n",
      "10318(-) -> https://www.isna.ir/news/98061005201/Ú©Ø§Ø±Ú¯Ø§Ù‡-ØªÙˆØ§Ù†Ù…Ù†Ø¯ÛŒ-Ø³Ø§Ø²ÛŒ-Ù…Ø¯ÛŒØ±Ø§Ù†-Ú©Ø§Ø±ÙˆØ§Ù†-Ù‡Ø§ÛŒ-Ø±Ø§Ù‡ÛŒØ§Ù†-Ù†ÙˆØ±-Ø¨Ø±Ú¯Ø²Ø§Ø±-Ø´Ø¯ \n",
      "10025(-) -> https://www.isna.ir/news/98041708948/Û²Ûµ-Ù‡Ø²Ø§Ø±-Ø¯Ø§Ù†Ø´-Ø¢Ù…ÙˆØ²-Ú©Ø±Ø¯Ø³ØªØ§Ù†ÛŒ-Ø¨Ù‡-Ø±Ø§Ù‡ÛŒØ§Ù†-Ù†ÙˆØ±-Ù…ÛŒ-Ø±ÙˆÙ†Ø¯ \n",
      "8518 (-) -> https://www.isna.ir/news/99070806230/Û±Ûµ-ÛŒØ§Ø¯Ù…Ø§Ù†-Ø¯ÙØ§Ø¹-Ù…Ù‚Ø¯Ø³-Ø¨Ù‡-Ø¹Ù†ÙˆØ§Ù†-Ø¢Ø«Ø§Ø±-Ù…Ù„ÛŒ-Ø¯ÙØ§Ø¹-Ù…Ù‚Ø¯Ø³-Ø«Ø¨Øª-Ø´Ø¯Ù‡-Ø§Ø³Øª \n",
      "10121(-) -> https://www.isna.ir/news/98050502469/Ø³ÙØ±-Ø¨ÛŒØ´-Ø§Ø²-Û±Û²-Ù‡Ø²Ø§Ø±-Ù†ÙØ±-Ø¨Ù‡-Ø§Ø±Ø¯ÙˆÙ‡Ø§ÛŒ-ÙØ±Ù‡Ù†Ú¯ÛŒ-ØªÙˆØ³Ø·-Ø³Ù¾Ø§Ù‡-Ù‚Ø²ÙˆÛŒÙ† \n",
      "9588 (-) -> https://www.isna.ir/news/98012208823/Ø±Ø´Ø¯-Û³Û°-Ø¯Ø±ØµØ¯ÛŒ-Ø§Ø¹Ø²Ø§Ù…-Ø²Ø§Ø¦Ø±Ø§Ù†-Ø±Ø§Ù‡ÛŒØ§Ù†-Ù†ÙˆØ±-Ø¯Ø±-Ú©Ø±Ù…Ø§Ù† \n",
      "11170(-) -> https://www.isna.ir/news/98111511016/Ø­Ø¶ÙˆØ±-Û²Ûµ-Ù‡Ø²Ø§Ø±-Ù†ÙØ±-Ø§Ø²-Ù…Ø§Ø²Ù†Ø¯Ø±Ø§Ù†-Ø¯Ø±-ÛŒØ§Ø¯Ù…Ø§Ù†-Ù‡Ø§ÛŒ-Ø¯ÙØ§Ø¹-Ù…Ù‚Ø¯Ø³ \n",
      "9312 (-) -> https://www.isna.ir/news/99112619254/Ø§Ø±Ø¯ÙˆÛŒ-Ø±Ø§Ù‡ÛŒØ§Ù†-Ù†ÙˆØ±-Ù…Ø¬Ø§Ø²ÛŒ-Ø¯Ø§Ù†Ø´-Ø¢Ù…ÙˆØ²Ø§Ù†-Ø§Ø±Ø¯Ø¨ÛŒÙ„-Ø¨Ø±Ú¯Ø²Ø§Ø±-Ù…ÛŒ-Ø´ÙˆØ¯ \n",
      "9514 (-) -> https://www.isna.ir/news/98010501105/Ù…Ù†Ø§Ø·Ù‚-Ø¹Ù…Ù„ÛŒØ§ØªÛŒ-Ø®ÙˆØ²Ø³ØªØ§Ù†-Ù‡Ù†ÙˆØ²-Ø¢Ù„ÙˆØ¯Ù‡-Ø¨Ù‡-Ù…ÛŒÙ†-Ù‡Ø³ØªÙ†Ø¯ \n",
      "11065(-) -> https://www.isna.ir/news/98102419036/Ø±Ø´Ø¯-Û²Û·-Ø¯Ø±ØµØ¯ÛŒ-Ø§Ø±Ø¯ÙˆÙ‡Ø§ÛŒ-Ø±Ø§Ù‡ÛŒØ§Ù†-Ù†ÙˆØ±-Ø¯Ø§Ù†Ø´-Ø¢Ù…ÙˆØ²ÛŒ-Ø§Ø³ØªØ§Ù†-Ú©Ø±Ù…Ø§Ù†Ø´Ø§Ù‡ \n",
      "10961(-) -> https://www.isna.ir/news/98100604120/Ú¯Ø²Ø§Ø±Ø´ÛŒ-Ø§Ø²-Ø­Ø¶ÙˆØ±-Ú¯ÛŒÙ„Ø§Ù†ÛŒ-Ù‡Ø§-Ø¯Ø±-Ù…Ù†Ø§Ø·Ù‚-Ø¹Ù…Ù„ÛŒØ§ØªÛŒ \n",
      "11415(-) -> https://www.isna.ir/news/98122720736/Ø´Ù‡Ø§Ø¯Øª-Ø³Ù‡-Ø®Ø§Ø¯Ù…-Ø±Ø§Ù‡ÛŒØ§Ù†-Ù†ÙˆØ±-Ø¯Ø±-Ø±Ø§Ù‡-Ù…Ø¨Ø§Ø±Ø²Ù‡-Ø¨Ø§-Ú©Ø±ÙˆÙ†Ø§ \n",
      "10223(-) -> https://www.isna.ir/news/98052311624/ØªØ´Ø±ÛŒØ­-Ø­Ø¶ÙˆØ±-Ú©Ø§Ø±ÙˆØ§Ù†-Ù‡Ø§ÛŒ-Ø±Ø§Ù‡ÛŒØ§Ù†-Ù†ÙˆØ±-Ø¯Ø±-ÛŒØ§Ø¯Ù…Ø§Ù†-Ù‡Ø§ÛŒ-Ø¯ÙØ§Ø¹-Ù…Ù‚Ø¯Ø³ \n",
      "10232(-) -> https://www.isna.ir/news/98052311624/ØªØ´Ø±ÛŒØ­-Ø­Ø¶ÙˆØ±-Ú©Ø§Ø±ÙˆØ§Ù†-Ù‡Ø§ÛŒ-Ø±Ø§Ù‡ÛŒØ§Ù†-Ù†ÙˆØ±-Ø¯Ø±-ÛŒØ§Ø¯Ù…Ø§Ù†-Ù‡Ø§ÛŒ-Ø¯ÙØ§Ø¹-Ù…Ù‚Ø¯Ø³ \n",
      "10090(-) -> https://www.isna.ir/news/98043015977/ØªØ´Ø±ÛŒØ­-Ø¬Ø²Ø¦ÛŒØ§Øª-Ø­Ø¶ÙˆØ±-Ù…Ø±Ø¯Ù…-Ø§Ø±Ø¯Ø¨ÛŒÙ„-Ø¯Ø±-ÛŒØ§Ø¯Ù…Ø§Ù†-Ù‡Ø§ÛŒ-Ø´Ù…Ø§Ù„-ØºØ±Ø¨-Ú©Ø´ÙˆØ± \n",
      "10800(-) -> https://www.isna.ir/news/98090604281/Ø¢Ù…Ø§Ø±ÛŒ-Ø§Ø²-Ø­Ø¶ÙˆØ±-Ø²Ø§Ø¦Ø±Ø§Ù†-Ø±Ø§Ù‡ÛŒØ§Ù†-Ù†ÙˆØ±-ØºØ±Ø¨-Ú©Ø´ÙˆØ± \n",
      "9258 (-) -> https://www.isna.ir/news/99111410631/Ø§Ù‡Ø¯Ø§ÛŒ-Ù†Ø´Ø§Ù†-Ù…Ù„ÛŒ-Ø®Ø§Ø¯Ù…ÛŒ-Ø´Ù‡Ø¯Ø§-Ø¨Ù‡-Ø³Ø±Ø¯Ø§Ø±-Ù…Ø·Ù‡Ø±ÛŒ \n",
      "10510(-) -> https://www.isna.ir/news/98071310002/Ø­Ø¶ÙˆØ±-Ù†Ø®Ø³ØªÛŒÙ†-Ú©Ø§Ø±ÙˆØ§Ù†-Ø±Ø§Ù‡ÛŒØ§Ù†-Ù†ÙˆØ±-Ø¯Ø§Ù†Ø´-Ø¢Ù…ÙˆØ²ÛŒ-Ø§Ø±Ø¯Ø¨ÛŒÙ„-Ø¯Ø±-ÛŒØ§Ø¯Ù…Ø§Ù†-Ù‡Ø§ÛŒ \n",
      "8530 (-) -> https://www.isna.ir/news/99071007594/ÙˆØ²Ø§Ø±Øª-ØªØ¹Ø§ÙˆÙ†-Ù…Ø³Ø¦ÙˆÙ„ÛŒØª-Ø³Ù‡-ÛŒØ§Ø¯Ù…Ø§Ù†-Ø¯ÙØ§Ø¹-Ù…Ù‚Ø¯Ø³-Ø±Ø§-Ø¯Ø§Ø±Ø¯ \n",
      "9712 (-) -> https://www.isna.ir/news/98021407389/Ø³Ø±Ø¯Ø§Ø±-Ø³Ù„ÛŒÙ…Ø§Ù†ÛŒ-Ù†Ù‚Ø´-Ù…Ø±Ø¯Ù…-Ø¯Ø±-Ø±Ø§Ù‡ÛŒØ§Ù†-Ù†ÙˆØ±-Ø¨Ø±Ø¬Ø³ØªÙ‡-Ø¨ÙˆØ¯ \n",
      "7863 (-) -> https://www.isna.ir/news/99040100559/Ø§Ø¹Ø²Ø§Ù…-Ú©Ø§Ø±ÙˆØ§Ù†-Ù‡Ø§ÛŒ-Ø±Ø§Ù‡ÛŒØ§Ù†-Ù†ÙˆØ±-ÙØ¹Ù„Ø§-Ù…Ù†ØªÙÛŒ-Ø´Ø¯Ù‡-Ø§Ø³Øª \n",
      "10723(-) -> https://www.isna.ir/news/98082113787/Ú¯Ø²Ø§Ø±Ø´ÛŒ-Ø§Ø²-ÙˆØ¶Ø¹ÛŒØª-Ø¨Ø§Ø²Ø¯ÛŒØ¯-Ø²Ø§Ø¦Ø±Ø§Ù†-Ø±Ø§Ù‡ÛŒØ§Ù†-Ù†ÙˆØ±-Ú©Ø±Ù…Ø§Ù†Ø´Ø§Ù‡ \n",
      "30800(-) -> https://www.farsnews.ir/news/14000123000337/Ø§Ø¹Ø²Ø§Ù…-Ú©Ù…Ø§Ù†Ø¯Ø§Ø±Ø§Ù†-Ø¨Ù‡-Ú©Ø§Ù¾-Ø¬Ù‡Ø§Ù†ÛŒ-Ù„ÙˆØ²Ø§Ù† \n",
      "10877(-) -> https://www.isna.ir/news/98092015284/Ú¯Ø±Ø§Ù…ÛŒØ¯Ø§Ø´Øª-Ø¹Ù…Ù„ÛŒØ§Øª-Ù…Ø·Ù„Ø¹-Ø§Ù„ÙØ¬Ø±-Ø¯Ø±-Ø§Ø±ØªÙØ§Ø¹Ø§Øª-Ú¯ÛŒÙ„Ø§Ù†ÙØ±Ø¨ \n",
      "10569(-) -> https://www.isna.ir/news/98072418585/Ù†Ù‚Ø´-Ø®Ø§Ø¯Ù…ÛŒÙ†-Ø´Ù‡Ø¯Ø§-Ø¯Ø±-Ø¨Ø±Ù¾Ø§ÛŒÛŒ-Ù…ÙˆÚ©Ø¨-Ù‡Ø§ÛŒ-Ù¾Ø°ÛŒØ±Ø§ÛŒÛŒ-Ø§Ø²-Ø²ÙˆØ§Ø±-Ø­Ø³ÛŒÙ†ÛŒ \n",
      "10859(-) -> https://www.isna.ir/news/98091712620/Ø³Ø§ÛŒÙ‡-Ú©Ø§Ù‡Ø´-Ø¨ÙˆØ¯Ø¬Ù‡-Ø¨Ø±-Ø³Ø±-ØªØ±ÙˆÛŒØ¬-ÙØ±Ù‡Ù†Ú¯-Ø¯ÙØ§Ø¹-Ù…Ù‚Ø¯Ø³ \n",
      "10420(-) -> https://www.isna.ir/news/98062612611/Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒ-Ø§Ø±Ø¯ÙˆÛŒÛŒ-Ø¯Ø±-Ø±Ø§Ø³ØªØ§ÛŒ-Ø¬Ø±ÛŒØ§Ù†-Ø´Ù†Ø§Ø³ÛŒ-Ù…Ù†Ø§ÙÙ‚ÛŒÙ†-Ø¯Ø±-Ù…Ø§Ø²Ù†Ø¯Ø±Ø§Ù† \n",
      "9512 (-) -> https://www.isna.ir/news/98010501130/Ø§Ø³Ú©Ø§Ù†-Û²Û°Û°Û°-Ø²Ø§Ø¦Ø±-Ø±Ø§Ù‡ÛŒØ§Ù†-Ù†ÙˆØ±-Ø¯Ø±-Ø§Ø±Ø¯ÙˆÚ¯Ø§Ù‡-Ù‡Ø§ÛŒ-Ø¢Ø¨Ø§Ø¯Ø§Ù† \n",
      "32583(-) -> https://www.farsnews.ir/news/13991229000552/Ø§Ø³Ø§Ù…ÛŒ-Ø¬ÙˆØ¯ÙˆÚ©Ø§Ø±Ø§Ù†-ØªÛŒÙ…-Ù…Ù„ÛŒ-Ø§Ø¹Ø²Ø§Ù…ÛŒ-Ø¨Ù‡-Ù‚Ù‡Ø±Ù…Ø§Ù†ÛŒ-Ø¢Ø³ÛŒØ§-Ø§Ø¹Ù„Ø§Ù…-Ø´Ø¯ \n",
      "9395 (-) -> https://www.isna.ir/news/99121107995/Ø§Ø³ØªÙØ§Ø¯Ù‡-Ø­Ø¯Ø§Ú©Ø«Ø±ÛŒ-Ø§Ø²-Ø¸Ø±ÙÛŒØª-ÙØ¶Ø§ÛŒ-Ù…Ø¬Ø§Ø²ÛŒ-Ø¨Ø±Ø§ÛŒ-Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒ-Ø§Ø±Ø¯ÙˆÙ‡Ø§ÛŒ-Ø±Ø§Ù‡ÛŒØ§Ù† \n",
      "9406 (-) -> https://www.isna.ir/news/99121410862/Ø­Ø¶ÙˆØ±-Ø¨ÛŒØ´ØªØ±-Ø§Ø²-Û²Û°-Ù‡Ø²Ø§Ø±-Ø¯Ø§Ù†Ø´-Ø¢Ù…ÙˆØ²-Ú¯Ù„Ø³ØªØ§Ù†ÛŒ-Ø¯Ø±-Ø±Ø§Ù‡ÛŒØ§Ù†-Ù†ÙˆØ±-Ù…Ø¬Ø§Ø²ÛŒ \n",
      "11082(-) -> https://www.isna.ir/news/98102821723/Ø­Ø¶ÙˆØ±-ÛµÛ°-Ù‡Ø²Ø§Ø±-Ù†ÙØ±ÛŒ-Ù…Ø±Ø¯Ù…-Ø¢Ø°Ø±Ø¨Ø§ÛŒØ¬Ø§Ù†-ØºØ±Ø¨ÛŒ-Ø¯Ø±-ÛŒØ§Ø¯Ù…Ø§Ù†-Ù‡Ø§ÛŒ-Ø¯ÙØ§Ø¹-Ù…Ù‚Ø¯Ø³ \n",
      "11163(-) -> https://www.isna.ir/news/98111409909/Ø¢Ø´Ù†Ø§-ÙˆØ¸ÛŒÙÙ‡-Ø¯ÙˆÙ„Øª-Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ-Ø§Ø²-Ø­Ø±Ú©Øª-ÙØ±Ù‡Ù†Ú¯ÛŒ-Ùˆ-Ù…Ø±Ø¯Ù…ÛŒ-Ø±Ø§Ù‡ÛŒØ§Ù†-Ù†ÙˆØ±-Ø§Ø³Øª \n",
      "11352(-) -> https://www.isna.ir/news/98121713243/ÙØ¹Ø§Ù„ÛŒØª-Ú¯Ø±ÙˆÙ‡-Ù‡Ø§ÛŒ-Ø¬Ù‡Ø§Ø¯ÛŒ-Ø³Ù…Ù†Ø§Ù†-Ø¨Ø±Ø§ÛŒ-Ù…Ù‚Ø§Ø¨Ù„Ù‡-Ø¨Ø§-ÙˆÛŒØ±ÙˆØ³-Ú©Ø±ÙˆÙ†Ø§ \n",
      "29085(-) -> https://www.farsnews.ir/news/14000207000678/ØªÛŒÙ…-Ù…Ù„ÛŒ-Ø¨ÙˆÚ©Ø³-Ø±Ø§Ù‡ÛŒ-Ø§Ø²Ø¨Ú©Ø³ØªØ§Ù†-Ù…ÛŒâ€ŒØ´ÙˆØ¯ \n",
      "9341 (-) -> https://www.isna.ir/news/99120201018/Ø­Ø¶ÙˆØ±-Ø¯Ø§Ù†Ø´-Ø¢Ù…ÙˆØ²Ø§Ù†-Ø¯Ø±-Ø±Ø§Ù‡ÛŒØ§Ù†-Ù†ÙˆØ±-Ù…Ø¬Ø§Ø²ÛŒ \n",
      "38782(-) -> https://www.farsnews.ir/news/13991107000374/ÛŒØ²Ø¯Ø§Ù†ÛŒ-Ø®ÛŒÙ„ÛŒ-Ø¯ÙˆØ³Øª-Ø¯Ø§Ø´ØªÙ…-Ø¯Ø±-ØµØ±Ø¨Ø³ØªØ§Ù†-Ø´Ø±Ú©Øª-Ú©Ù†Ù…-Ø´Ø±Ø§ÛŒØ·Ù…-Ù¾Ø³-Ø§Ø²-Ú©Ø±ÙˆÙ†Ø§-Ø®ÙˆØ¨ \n",
      "8925 (-) -> https://www.isna.ir/news/99091310235/Ø§Ù‡Ø¯Ø§ÛŒ-Ù„ÙˆØ­-Ùˆ-Ù†Ø´Ø§Ù†-Ø®Ø§Ø¯Ù…ÛŒ-Ø´Ù‡Ø¯Ø§-Ø¨Ù‡-ÙØ±Ù…Ø§Ù†Ø¯Ù‡-Ù…Ø±Ø²Ø¨Ø§Ù†ÛŒ-Ù†ÛŒØ±ÙˆÛŒ-Ø§Ù†ØªØ¸Ø§Ù…ÛŒ \n",
      "38662(-) -> https://www.farsnews.ir/news/13991108000262/Ø§Ø±Ø¯ÙˆÛŒ-ØªÛŒÙ…-Ù…Ù„ÛŒ-ÙˆØ§Ù„ÛŒØ¨Ø§Ù„-Ø³Ø§Ø­Ù„ÛŒ-ØªØ¹Ø·ÛŒÙ„-Ø´Ø¯-Ú©Ø±ÙˆÙ†Ø§-Ú©Ø§Ø±-Ø¯Ø³Øª-Ø³Ø§Ø­Ù„ÛŒâ€ŒØ¨Ø§Ø²Ø§Ù†-Ø¯Ø§Ø¯ \n",
      "10127(-) -> https://www.isna.ir/news/98050603092/Ø­Ø¶ÙˆØ±-Ø§Ù‚Ø´Ø§Ø±-Ù…Ø®ØªÙ„Ù-Ù…Ø±Ø¯Ù…-Ø¯Ø±-ÛŒØ§Ø¯Ù…Ø§Ù†-Ù‡Ø§ÛŒ-Ø¯ÙØ§Ø¹-Ù…Ù‚Ø¯Ø³-Ø¢Ø°Ø±Ø¨Ø§ÛŒØ¬Ø§Ù†-ØºØ±Ø¨ÛŒ \n",
      "34298(-) -> https://www.farsnews.ir/news/13991214000401/ØªÙ…Ø±ÛŒÙ†-Ù…Ø´ØªØ±Ú©-ØªÛŒÙ…â€ŒÙ‡Ø§ÛŒ-Ø¨ÙˆÚ©Ø³-Ø¬ÙˆØ§Ù†Ø§Ù†-Ø§ÛŒØ±Ø§Ù†-Ùˆ-Ø¢Ø°Ø±Ø¨Ø§ÛŒØ¬Ø§Ù†-ÙÛŒÙ„Ù… \n",
      "33711(-) -> https://www.farsnews.ir/news/13991219000118/Ø¢ØºØ§Ø²-Ø§Ø±Ø¯ÙˆÛŒ-ØªÛŒÙ…-Ù…Ù„ÛŒ-Ø¬ÙˆØ¯Ùˆ-Ù¾Ø³-Ø§Ø²-Ù¾Ø§ÛŒØ§Ù†-Ø³ÙˆÙ¾Ø±-Ù„ÛŒÚ¯ \n"
     ]
    }
   ],
   "source": [
    "query_responding(100,4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
