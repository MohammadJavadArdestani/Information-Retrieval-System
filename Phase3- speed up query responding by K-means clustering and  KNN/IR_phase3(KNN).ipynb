{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import datetime\n",
    "import collections\n",
    "import math\n",
    "import heapq\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## marks lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_marks = [')','(','>','<',\"ÿõ\",\"ÿå\",'{','}',\"ÿü\",':',\"-\", '¬ª', '\"', '¬´', '[', ']','\"','+','=','?']\n",
    "marks = ['/','//', '\\\\','|','!', '%', '&','*','$', '#','ÿü', '*','.','_' ]\n",
    "alphabet_string_lower = string.ascii_lowercase\n",
    "alphabet_string_upper = string.ascii_uppercase\n",
    "english_char =  list(alphabet_string_lower) + list(alphabet_string_upper)\n",
    "\n",
    "\n",
    "sep_list = [\" \", '\\xad', '\\u200e','\\u200f', '\\u200d', '\\u200d', '\\u200d'] + marks\n",
    "\n",
    "# stop_words1 =['ÿ®Ÿá', 'Ÿà', 'ÿØÿ±', 'ÿ®ÿß', 'ÿß€åŸÜ', 'ÿ¥ÿØ', 'ÿ±ÿß', '⁄©Ÿá', 'ÿßÿ≤', '⁄©Ÿá', 'ÿß€åŸÜ', 'ÿ®ÿß', 'ÿßÿ≥ÿ™', 'ÿ®ÿ±ÿß€å', 'ÿ¢ŸÜ', '€å⁄©', 'ÿÆŸàÿØ', 'ÿ™ÿß', '⁄©ÿ±ÿØ', 'ÿ®ÿ±', 'ŸáŸÖ', 'ŸÜ€åÿ≤', 'Ÿáÿ≤ÿßÿ±', 'ÿ±€åÿßŸÑ', 'ÿ®ŸàÿØ']\n",
    "stop_words = [\"ÿ®ÿ±ÿß€å\", \"Ÿæÿ≥\", \"ÿ≥Ÿæÿ≥\", \"ÿ™ÿß\", \"ÿßÿ≤\", \"⁄©Ÿá\", \"Ÿà\", \"ÿ®Ÿá\", \"ÿ±ÿß\", \"ÿ®ÿß\", \"ÿ®ÿ±\", \"ÿØÿ±\", \"ÿß€åŸÜ⁄©Ÿá\", \"ÿß€åŸÜ\", \"ÿßŸÜ\", \"⁄Üÿ±ÿß\",\n",
    "              \"ÿ¥ÿß€åÿØ\", \"ÿßŸÜŸáÿß\", \"⁄ÜŸàŸÜ\", \"ÿßŸÜÿ∑Ÿàÿ±\", \"ÿß€åŸÜÿ∑Ÿàÿ±\", \"ÿßŸÜ⁄ÜŸá\", \"ÿ¢ÿÆ\", \"ÿ¢ÿÆÿ±\", \"ÿ¢ÿÆÿ±Ÿáÿß\", \"ÿßÿÆŸá\", \"ÿ¢ÿ±Ÿá\", \"ÿ¢ÿ±€å\", \"ÿßŸÜÿßŸÜ\",\n",
    "              \"ÿß⁄Øÿ±\", \"ÿØÿ±ÿ®ÿßÿ±Ÿá\", \"ÿÆ€åŸÑ€å\", \"ÿ™Ÿà€å\", \"ÿ®ŸÑ⁄©Ÿá\", \"ÿ®ÿπÿ∂€å\", \"ÿ®ÿπÿØÿß\", \"ÿ®ÿß€åÿØ\", \"ÿßŸÑÿ®ÿ™Ÿá\", \"ÿßŸÑÿßŸÜ\", \"ÿßÿµŸÑÿß\", \"ÿßÿ≥ÿßÿ≥ÿß\",\n",
    "              \"ÿßÿ≠ÿ™ŸÖÿßŸÑÿß\", \"ÿßÿÆ€åÿ±ÿß\", \"ÿß€åÿß\", \"ÿßŸáÿßŸÜ\", \"ÿßŸÖÿ±ÿßŸÜŸá\", \"ÿßŸÜ ⁄ØÿßŸá\", \"ÿßŸÜÿ±ÿß\", \"ÿßŸÜŸÇÿØÿ±\", \"ÿßÿ≥ÿßÿ≥ÿß\", \"ÿ®ŸÜÿßÿ®ÿ±ÿß€åŸÜ\", \"Ÿáÿ±ÿ≠ÿßŸÑ\",\n",
    "              \"ÿ®€å‚ÄåÿßŸÜ⁄©Ÿá\", \"ÿ®Ÿá‚ÄåŸÇÿØÿ±€å\", \"ÿ®€åÿ¥ÿ™ÿ±\", \"⁄©ŸÖÿ™ÿ±\", \"ÿ™ÿ±\", \"ÿ™ÿ±€åŸÜ\", \"ÿ™Ÿà\", \"⁄ÜŸÜÿØ\", \"⁄ÜŸÜÿØ€åŸÜ\", \"⁄ÜŸá\", \"ÿÆ€åŸÑ€å\", \"ÿØ€å⁄Øÿ±\", \"ÿØ€å⁄ØŸá\",\n",
    "              \"ÿ∑ÿ®€åÿπÿ™ÿß\", \"ÿπŸÖÿØÿß\", \"⁄ØÿßŸá€å\", \"ŸÜ€åÿ≤\", \"ÿßŸà\", \"ÿ™Ÿà\", \"ŸÖŸÜ\", \"ŸÖÿß\", \"ÿ∑Ÿàÿ±\", \"ÿØŸà\", \"Ÿáÿ±\", \"ŸáŸÖŸá\", \"ÿß€åŸÖ\", \"ÿßŸÜÿØ\", \"ÿß€åÿØ\",\n",
    "              \"ÿßÿ≥ÿ™\", \"Ÿáÿ≥ÿ™\", \"ÿ≤ÿØ⁄Ø€å\", \"ÿÆÿµŸàÿµÿß\", \"ŸÜ\", \"ÿ®\", \"⁄©ÿ≥€å\", \"⁄Ü€åÿ≤€å\", \"ÿ®ÿßŸÑÿßÿÆÿ±Ÿá\", \"ŸàŸÇÿ™€å\", \"ÿ≤ŸÖÿßŸÜ€å\", \"ŸÖ€å\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_word_data_20k = ['', 'Ÿæ€åÿßŸÖ', 'ÿÆŸàÿßŸá','ÿØÿßÿ±', 'ÿ¥ÿØŸá', '€µ', '⁄Øÿ≤ÿßÿ±', 'Ÿáÿß', '⁄Ø€åÿ±', 'ŸÇÿ±ÿßÿ±', 'ÿßŸÜÿ™', '€¥', 'ÿ≠ÿ∂Ÿàÿ±', 'Ÿà€å', 'ŸÖ€å',  'ÿßŸÜ', 'ÿ®€åŸÜ', 'ÿ≥ÿßŸÑ', '€∑', '⁄ØŸÅÿ™', '⁄©ÿßÿ±', '€å', '⁄©ÿ±ÿØŸá', 'ÿßŸÖÿß', 'ÿßÿØÿßŸÖŸá', 'ÿ¥Ÿà', 'ÿ±Ÿàÿ≤', '⁄©ŸÜ', '⁄Øÿ∞ÿ¥ÿ™Ÿá', 'ÿØÿßÿ¥ÿ™', 'Ÿáÿß€å', 'ÿ®ÿßÿ≤', 'ÿßŸÜÿ¨ÿßŸÖ', 'ÿØÿßÿØ', '€≥', 'ÿ®ÿß€åÿØ', 'ÿß€åŸÜ⁄©Ÿá', 'Ÿæ€åÿ¥', 'ÿ®€åÿ¥', 'ÿØŸà', 'ÿ®ÿßÿ¥', 'ÿ™ŸàÿßŸÜ', 'ÿØ€å⁄Øÿ±', '€≤', 'ÿπŸÜŸà',  'ÿßŸÖÿ±Ÿàÿ≤', '€π', '€∏', 'Ÿæÿß€å', 'ŸÖŸàÿ±ÿØ', 'ÿµŸàÿ±ÿ™', 'ŸÖÿ±ÿØŸÖ', 'ÿ®', 'Ÿáÿ±', 'ŸÖÿß', 'ÿß€åÿ±', 'ÿ±Ÿà', 'ÿßÿ∏Ÿáÿßÿ±', '€±', 'ÿßŸÅÿ≤ŸàÿØ', '‚Äì', '€∂', 'ÿß', 'ÿ±', 'ÿ´',  '€∞', 'ÿ¥', 'ÿØ', '⁄ò', '⁄©', 'ÿπ', 'Ÿá', 'ÿ¨', 'ÿ≤', 'ŸÖ', 'ÿ™', 'Ÿ¢', 'Ÿ£', 'Ÿß', 'ŸÇ', 'ŸÑ', 'ŸÜ', '@', ',', '\\u2066', 'Ÿ©', 'Ÿ§', 'Ÿ•', 'Ÿ®', 'Ÿ°', 'Ÿ¶', 'ÿ≥', 'ÿµ', 'ÿ°', 'ŸÅ', \"'\", '√∂', '⁄Ø', '„Äã', 'ÿ∑', 'Ÿæ', '‚Ä¶', 'ÿÆ', '√™', 'ÿ≠', '\\u2067', '\\u2069', '€ó', '‚Ä¢', 'Ÿ™', '\\u200b', '√ó', '€ö', '€ñ', '⁄Ü', '\"', '\\u2063', 'ÿ∞', '\\uf0d8', '€î', '√º', 'Ôª≠', 'ÿ∫', 'Ô∏è', 'Ô∑≤', '\\u202c', '\\u202a', 'Ôª´', '€ô', 'ÿ∂', '¬∑', '¬¨', ';', 'ÿÆÿ®ÿ± ⁄Øÿ≤ÿßÿ±€å','⁄Øÿ≤ÿßÿ≤ÿ¥','ÿÆÿ®ÿ±ŸÜ⁄Øÿßÿ±',]\n",
    "# # stop_word_data_17k = ['', 'ÿØÿ±ÿµÿØ', 'Ÿà€å', '⁄©ŸÜ', 'ÿ™ŸàÿßŸÜ', 'Ÿæ€åÿßŸÖ', 'ŸÖÿß', '⁄ØŸÅÿ™', 'ÿØŸàŸÑÿ™', 'ÿÆŸàÿßŸá', 'ÿ®ÿß€åÿØ', 'ÿß€åŸÜ⁄©Ÿá','ÿßŸÜ', 'ÿ™ŸàŸÑ€åÿØ', 'ÿØÿßÿ±', 'ÿß⁄Øÿ±', 'Ÿáÿ≥ÿ™ŸÜÿØ', , 'ÿ¥Ÿà', 'Ÿæ€åÿ¥', 'Ÿàÿ¨ŸàÿØ','ÿß€åÿ±', 'ÿ¥ÿØŸá', '⁄©ÿßÿ±', 'ÿ®ÿ±ÿÆ€å', 'ÿ±Ÿà', 'ÿ®€åŸÜ', 'ÿ™Ÿàÿ≥ÿ∑', '⁄Ø€åÿ±', 'ŸÜÿ∏ÿ±', 'ÿ®ÿßÿ¥', 'ŸáŸÖ⁄ÜŸÜ€åŸÜ', 'ÿ≥ÿßŸÑ', 'ÿØŸà', 'ÿØÿßÿ¥ÿ™', 'ÿßŸÜÿ¨ÿßŸÖ', 'ÿßŸÅÿ≤ÿß€å', 'ÿßŸÖÿß', 'Ÿáÿ±', 'ÿ≠ÿßŸÑ', 'ŸÇÿ±ÿßÿ±', 'ŸÖŸàÿ±ÿØ', 'ÿßŸÅÿ≤ŸàÿØ', '⁄Øÿ±ŸÅÿ™Ÿá',  'ÿßŸÜŸáÿß', 'ÿßŸÜÿ™', 'ÿØŸÑ€åŸÑ', 'ÿ®', 'ÿßÿØÿßŸÖŸá', 'ŸÖ€å', 'ÿ¥ÿ±⁄©ÿ™', 'ÿßÿ≥ÿßÿ≥', 'Ÿæÿß€å', '⁄Øÿ≤ÿßÿ±', '⁄©ÿ±ÿØŸá', 'ÿ≥ÿßÿ≤', 'ÿ®ÿÆÿ¥', 'ÿ®€åÿ¥', 'ÿØÿßÿØ', 'ŸÖŸàÿ∂Ÿàÿπ', 'ÿ±Ÿàÿ≤', '€åÿß', 'ÿ¥ÿ±ÿß€åÿ∑', 'ÿ®ŸàÿØŸá',  'ŸáŸÖ€åŸÜ', '⁄©ÿßŸáÿ¥', '⁄Øÿ∞ÿ¥ÿ™Ÿá', 'ÿ™Ÿàÿ¨Ÿá', '€å⁄©€å', 'ÿßŸÖÿ±Ÿàÿ≤', 'ŸÖÿßŸá', 'ÿ≤ÿßÿ±', 'ÿßŸÇÿØÿßŸÖ', '€µ', 'ÿØÿßÿ¥ÿ™Ÿá', '€≤', 'ÿπŸÜŸà', 'ÿØÿßÿØŸá', 'Ÿáÿß€å', 'ÿØ€å⁄Øÿ±', 'ŸÜÿ≥ÿ®ÿ™', 'ÿ®€åÿßŸÜ', 'ŸáŸÖŸá', '€∏', 'ÿßÿπŸÑÿßŸÖ',  'ŸÜŸÇŸÑ', 'Ÿæÿ≥', '€∑', 'ŸÖ€åŸÑ€åŸàŸÜ', '€π', 'ÿ±', '€∂', '€¥', '€≥', 'Ÿá', '€∞', '⁄Øÿ±ŸàŸá', 'ÿßÿ¥ÿßÿ±Ÿá', '€±€≥€π€π', '€±', 'ÿ¨', 'ŸÖÿ±ÿØŸÖ', 'ÿßÿ≥ÿ™ŸÅÿßÿØŸá', '€å', 'ÿ™', 'Ÿ•', 'Ÿ¢', 'Ÿ°', 'Ÿ£', 'Ÿ¶', 'ÿØ', '‚Äì', '⁄ò', 'ÿµ', 'ÿπ', 'ŸÖ', 'Ÿæ', 'ÿß', 'Ÿ§', 'ÿ¥', 'Ÿß', 'Ÿ©', '⁄©', 'ÿ≤', 'ŸÅ', 'ÿ≠', '‚Äô', 'ŸÜ', ',', 'Ÿ®', \"'\", '⁄Ü', 'Ÿ†', '‚Ä¢', '¬∑', '\\u200b', 'ÿ∫', '‚Ä¶', 'ŸÑ', 'ÿÆ', 'ÿ∞', 'ÿ´', '\\uf0d8', 'ÿ≥', '@', 'ŸÇ', 'ÿ°', 'ÿ∑', '⁄Ø', ';', 'Ÿ¨', '€ó', '\\uf0fc', '‚Äî', '√±', '√°', 'Ôª´', '√ó', '‚àí', '¬±', '\"', 'Ô¥æ', '¬∞', 'Ôª≠',  'üîπ', '€ö', '€ñ', '\\u202b', '≈ü', '√©', 'ÿ©', '€ô']\n",
    "# stop_word_data_11k = ['', 'Ÿæ€åÿßŸÖ', 'ÿß€åŸÜ⁄©Ÿá', 'ÿß€åÿ±', 'ÿ¥ÿØŸá', '⁄©ŸÜ', 'ÿ®ÿßÿ¥', '⁄©ÿ¥Ÿàÿ±', 'Ÿáÿß€å', '⁄©ÿßÿ±', 'ÿ±Ÿàÿ≤', 'ÿßÿπŸÑÿßŸÖ', '⁄Øÿ≤ÿßÿ±', 'ŸáŸÖ⁄ÜŸÜ€åŸÜ', 'ÿ≥ÿßŸÑ', '⁄©ÿ±ÿØŸá', 'ÿØÿßÿØ', 'ÿßŸÜÿ™', 'ÿπŸÜŸà', '€≥', 'ÿ≠ÿ∂Ÿàÿ±', 'ÿÆŸàÿßŸá', 'Ÿà€å', 'ÿßŸÅÿ≤ŸàÿØ', '⁄ØŸÅÿ™', 'ÿ®€åÿßŸÜ', 'ÿ™ŸàÿßŸÜ', 'ÿ¥Ÿà', 'ÿ®ÿÆÿ¥', 'ÿ®ÿß€åÿØ', 'ÿØÿßÿ±', 'ÿØÿßÿ¥ÿ™', 'ÿßŸÜ', 'ÿ™Ÿàÿ¨Ÿá', 'ŸÖŸàÿ±ÿØ', 'ÿØŸà', 'ÿßŸÜÿ¨ÿßŸÖ', '€≤', 'ÿ±Ÿà', 'Ÿáÿ±', '€åÿß', 'ÿµŸàÿ±ÿ™', 'Ÿáÿ≥ÿ™ŸÜÿØ', 'ÿßŸÖÿß', '€å', 'Ÿ¢', '⁄Øÿ∞ÿ¥ÿ™Ÿá', '€¥', 'ŸÇÿ±ÿßÿ±', 'ÿ®€åÿ¥', 'ÿØ€å⁄Øÿ±', 'ÿßÿØÿßŸÖŸá', 'ÿßŸÖÿ±Ÿàÿ≤', '€∞', '€∂', '€µ', '€π', '€∑', '€∏', '€±', 'ÿß', 'Ÿ£', 'ÿ®', 'ÿ≥', '‚Äì', 'Ÿ®', 'ÿ¨', 'Ÿß', 'Ÿæ', \"'\", 'ÿ≤', 'ÿ±', 'Ÿá', 'ÿπ', 'ÿ°', 'ŸÖ', '\\u202c', 'ÿ¥', 'Ÿ¶', 'ŸÜ', 'Ÿ©', '@', '€ö', 'Ÿ§', 'ÿØ', 'ÿµ', '⁄©', '\\u202b', 'ÿ´', '⁄Ü', 'ŸÑ', 'ÿ∑', 'ÿ≠', 'ŸÅ', 'ÿ™', '‚Ä¶', 'Ÿ•', '\\u2067', 'Ÿ°', 'ÿ∞', 'ŸÇ', '‚Äî', 'Ÿ†', ',', 'ÿ∫', '\\ufeff', 'Ÿ™', '\"', '\\u2069', '‚Ä¢', '€ó', '‚Äô', '⁄ò', 'ÿÆ', '–∏', '–û', '—Å', '–±', '–Ø', '–°', '–≤', '€î', '¬∑','ÿ®ÿ±','ÿ™ÿß','ÿØÿ±','ÿ®Ÿá','ÿßÿ≤', '⁄Ø', '√ñ', '√©', ';', '\\u206b', '„Äã', '‚†Ä', '\\u202a', '€ñ', '‚ùá', 'Ôª´'] + stop_words1\n",
    "# stop_words = set(stop_word_data_20k + stop_word_data_11k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regex  patterns, prefixe list and postfix list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "raw_postfix = [\n",
    "        ('[\\u200c](ÿ™ÿ±(€åŸÜ?)?|⁄Øÿ±€å?|Ÿáÿß€å?)' , ''),\n",
    "        (r'(ÿ™ÿ±(€åŸÜ?)?|⁄Øÿ±€å?)(?=$)' , ''), #ÿ™ÿ± ÿå ÿ™ÿ±€åŸÜÿå ⁄Øÿ±ÿå ⁄Øÿ±€å ÿØÿ± ÿ¢ÿÆÿ± ⁄©ŸÑŸÖŸá\n",
    "       # (r'(?<=[^ ÿßŸà])(ŸÖ|ÿ™|ÿ¥|ŸÖÿßŸÜ|ÿ™ÿßŸÜ|ÿ¥ÿßŸÜ|€å)$' , ''), # ÿ≠ÿ∞ŸÅ ÿ¥ŸÜÿßÿ≥Ÿá Ÿáÿß€å ŸÖÿßŸÑ⁄©€åÿ™ Ÿà ŸÅÿπŸÑ ÿØÿ± ÿ¢ÿÆÿ± ⁄©ŸÑŸÖŸá\n",
    "        (r'(?<=[^ ÿßŸà])(ŸÖ|ÿ™|ÿ¥|ŸÖÿßŸÜ|ÿ™ÿßŸÜ|ÿ¥ÿßŸÜ)$' , ''), # ÿ≠ÿ∞ŸÅ ÿ¥ŸÜÿßÿ≥Ÿá Ÿáÿß€å ŸÖÿßŸÑ⁄©€åÿ™ Ÿà ŸÅÿπŸÑ ÿØÿ± ÿ¢ÿÆÿ± ⁄©ŸÑŸÖŸá\n",
    "        (r'(ÿßŸÜ|ÿßÿ™|⁄Ø€å|⁄Øÿ±€å|Ÿáÿß€å)$' , ''), #ÿßŸÜÿå ÿßÿ™ÿå Ÿáÿßÿå Ÿáÿß€å ÿ¢ÿÆÿ± ⁄©ŸÑŸÖŸá  \n",
    "]\n",
    "\n",
    "raw_arabic_notation = [\n",
    "    # remove FATHATAN, DAMMATAN, KASRATAN, FATHA, DAMMA, KASRA, SHADDA, SUKUN\n",
    "    ('[\\u064B\\u064C\\u064D\\u064E\\u064F\\u0650\\u0651\\u0652]', ''),\n",
    "    ]\n",
    "\n",
    "raw_long_letters = [\n",
    "    (r' +', ' '),  # remove extra spaces\n",
    "    (r'\\n\\n+', '\\n'),  # remove extra newlines\n",
    "    (r'[ŸÄ\\r]', ''),  # remove keshide, carriage returns\n",
    "    ]\n",
    "\n",
    "raw_half_space = [\n",
    "    ('[\\u200c]', ''),\n",
    "]\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## verbs, mokassar,morakab list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "present_roots =['ÿ™ŸàÿßŸÜ','ÿ®ÿßÿ¥','ÿ±Ÿà','ÿ®ÿ±','€åÿßŸàÿ±', '€åÿßŸÜÿØÿßÿ≤', '€åÿß€å','€åÿßŸÜÿØ€åÿ¥','ÿ®ÿÆÿ¥','ÿ®ÿßÿ≤','ÿÆÿ±','ÿ®€åŸÜ','ÿ¥ŸÜŸà','ÿØÿßÿ±','ÿØÿßŸÜ','ÿ±ÿ≥ÿßŸÜ','ÿ¥ŸÜÿßÿ≥','⁄ØŸà','⁄Øÿ∞ÿßÿ±','€åÿßÿ®','ŸÑÿ±ÿ≤','ÿ≥ÿßÿ≤','ÿ¥Ÿà','ŸÜŸà€åÿ≥','ÿÆŸàÿßŸÜ','⁄©ÿßŸá','⁄Ø€åÿ±','ÿÆŸàÿßŸá','⁄©ŸÜ' ]\n",
    "\n",
    "past_roots = ['ÿ™ŸàÿßŸÜÿ≥ÿ™','ÿ®ŸàÿØ','⁄©ÿ±ÿØ','ÿßŸàÿ±ÿØ','ÿßŸÜÿØÿßÿÆÿ™','ÿßŸÖÿØ','ÿÆÿ±€åÿØ','ÿ®ÿßÿÆÿ™','ÿ®ÿ±ÿØ','ÿ±ŸÅÿ™','ÿßŸÜÿØ€åÿ¥€åÿØ','ÿ®ÿÆÿ¥€åÿØ','ÿØ€åÿØ','ÿ¥ŸÜ€åÿØ','ÿØÿßÿ¥ÿ™','ÿØÿßŸÜÿ≥ÿ™','ÿ±ÿ≥ÿßŸÜÿØ','ÿ¥ŸÜÿßÿÆÿ™','⁄ØŸÅÿ™','⁄Øÿ∞ÿ¥ÿ™','€åÿßŸÅÿ™','ŸÑÿ±ÿ≤€åÿØ','ÿ≥ÿßÿÆÿ™','ÿ¥ÿØ','ŸÜŸàÿ¥ÿ™','ÿÆŸàÿßŸÜÿØ','⁄©ÿßÿ≥ÿ™','⁄Øÿ±ŸÅÿ™','ÿÆŸàÿßÿ≥ÿ™']\n",
    "all_verbs_roots = present_roots + past_roots\n",
    "empty_list = ['','','','','','','']\n",
    "verb_prefix = ['ŸÜŸÖ€å‚Äå', 'ŸÖ€å‚Äå','ŸÜ','ÿ®',\"\" ]\n",
    "present_verb_postfix = ['ŸÖ','€å','ÿØ','€åÿØ','ŸÜÿØ','€åŸÖ']\n",
    "past_verb_postfix = ['ÿß€åŸÖ','ÿß€åÿØ','ÿß€å','ÿßŸÖ','ÿßŸÜÿØ']\n",
    "past_verb_postfix2 = ['ŸÖ','€å','€åÿØ','ŸÜÿØ','€åŸÖ']\n",
    "present_verbs = []\n",
    "past_verbs = []\n",
    "all_verbs = {}\n",
    "for pref in verb_prefix:\n",
    "    for present_root, past_root in zip(present_roots, past_roots):\n",
    "        for post in past_verb_postfix2:\n",
    "            all_verbs[pref + past_root+post] = past_root\n",
    "        for post in past_verb_postfix:\n",
    "            all_verbs[past_root+'Ÿá‚Äå'+post] = past_root\n",
    "        for post in present_verb_postfix:\n",
    "            all_verbs[pref+present_root+post] = present_root\n",
    "            \n",
    "words_list = ['ÿßÿØÿ®', 'ÿ¢ÿØÿßÿ®', 'ÿ∑ÿ±ŸÅ', 'ÿßÿ∑ÿ±ÿßŸÅ', 'ÿ≠ŸÇ€åŸÇÿ™', 'ÿ≠ŸÇÿß€åŸÇ', 'ŸÖŸàÿ¨', 'ÿßŸÖŸàÿßÿ¨', 'ÿßÿØ€åÿ®', 'ÿßÿØÿ®ÿß', 'ÿπŸÖŸÇ', 'ÿßÿπŸÖÿßŸÇ', 'ÿÆÿ≤€åŸÜŸá', 'ÿÆÿ≤ÿßÿ¶ŸÜ', 'ŸÖÿ±⁄©ÿ≤', 'ŸÖÿ±ÿß⁄©ÿ≤', 'ÿßÿ´ÿ±', 'ÿ¢ÿ´ÿßÿ±', 'ÿπÿßŸÑŸÖ', 'ÿπŸÑŸÖÿß', 'ÿÆÿ®ÿ±', 'ÿßÿÆÿ®ÿßÿ±', 'ŸÖŸàŸÇÿπ', 'ŸÖŸàÿßŸÇÿπ', 'ÿßÿ≥€åÿ±', 'ÿßÿ≥ÿ±ÿß', 'ÿπŸÑŸÖ', 'ÿπŸÑŸàŸÖ', 'ÿØŸàÿ±Ÿá', 'ÿßÿØŸàÿßÿ±', 'ŸÖÿµÿ±ŸÅ', 'ŸÖÿµÿßÿ±ŸÅ', 'ÿßÿ≥ŸÖ', 'ÿßÿ≥ÿßŸÖ€å', 'ÿπŸÑÿßŸÖÿ™', 'ÿπŸÑÿßÿ¶ŸÖ', 'ÿØ€åŸÜ', 'ÿßÿØ€åÿßŸÜ', 'ŸÖÿπÿ±ŸÅÿ™', 'ŸÖÿπÿßÿ±ŸÅ', 'ÿßÿ≥ÿ∑Ÿàÿ±Ÿá', 'ÿßÿ≥ÿßÿ∑€åÿ±', 'ÿπŸÑÿ™', 'ÿπŸÑŸÑ', 'ÿØŸÅÿ™ÿ±', 'ÿØŸÅÿßÿ™ÿ±', 'ŸÖÿ®ÿ≠ÿ´', 'ŸÖÿ®ÿßÿ≠ÿ´', 'ÿßŸÖ€åÿ±', 'ÿßŸÖÿ±ÿß', 'ÿπŸÇ€åÿØŸá', 'ÿπŸÇÿßÿ¶ÿØ', 'ÿ∞ÿÆ€åÿ±Ÿá', 'ÿ∞ÿÆÿß€åÿ±', 'ŸÖÿßÿØŸá', 'ŸÖŸàÿßÿØ', 'ÿßŸÖÿ±', 'ÿßŸàÿßŸÖÿ±', 'ÿπŸÖŸÑ', 'ÿßÿπŸÖÿßŸÑ', 'ÿ±ŸÅ€åŸÇ', 'ÿ±ŸÅŸÇÿß', 'ŸÖÿ∞Ÿáÿ®', 'ŸÖÿ∞ÿßŸáÿ®', 'ÿßŸÖÿßŸÖ', 'ÿßÿ¶ŸÖŸá', 'ÿπ€åÿØ', 'ÿßÿπ€åÿßÿØ', 'ÿ±ÿß€å', 'ÿ¢ÿ±ÿß', 'ŸÖÿµ€åÿ®ÿ™', 'ŸÖÿµÿßÿ¶ÿ®', 'ÿßÿµŸÑ', 'ÿßÿµŸàŸÑ', 'ÿπŸÜÿµÿ±', 'ÿπŸÜÿßÿµÿ±', 'ÿ±ÿ≥ŸÖ', 'ÿ±ÿ≥ŸàŸÖ', 'ŸÖÿπÿ®ÿØ', 'ŸÖÿπÿßÿ®ÿØ', 'ÿßŸÅŸÇ', 'ÿ¢ŸÅÿßŸÇ', 'ÿπÿßÿ∑ŸÅŸá', 'ÿπŸàÿßÿ∑ŸÅ', 'ÿ±ÿßÿ®ÿ∑Ÿá', 'ÿ±Ÿàÿßÿ®ÿ∑', 'ŸÖÿ≥ÿ¨ÿØ', 'ŸÖÿ≥ÿßÿ¨ÿØ', 'ÿ®€åÿ™', 'ÿßÿ®€åÿßÿ™', 'ÿπÿ∂Ÿà', 'ÿßÿπÿ∂ÿß', 'ÿ±ŸÖÿ≤', 'ÿ±ŸÖŸàÿ≤', 'ŸÖÿπÿ®ÿ±', 'ŸÖÿπÿßÿ®ÿ±', 'ÿ™ÿßÿ¨ÿ±', 'ÿ™ÿ¨ÿßÿ±', 'ÿπÿ®ÿßÿ±ÿ™', 'ÿπÿ®ÿßÿ±ÿßÿ™', 'ÿ±ÿ¨ŸÑ', 'ÿ±ÿ¨ÿßŸÑ', 'ŸÖÿ∏Ÿáÿ±', 'ŸÖÿ∏ÿßŸáÿ±', 'ÿ™ÿµŸà€åÿ±', 'ÿ™ÿµÿßŸà€åÿ±', 'ÿπÿ¨€åÿ®', 'ÿπÿ¨ÿß€åÿ®', 'ÿ±ŸÇŸÖ', 'ÿßÿ±ŸÇÿßŸÖ', 'ŸÖŸÜÿ∏ÿ±Ÿá', 'ŸÖŸÜÿßÿ∏ÿ±', 'ÿ¨ÿØ', 'ÿßÿ¨ÿØÿßÿØ', 'ŸÅŸÇ€åŸá', 'ŸÅŸÇŸáÿß', 'ÿ≤ÿßŸà€åŸá', 'ÿ≤Ÿàÿß€åÿß', 'ŸÖÿ±ÿ∂', 'ÿßŸÖÿ±ÿßÿ∂', 'ÿ¨ÿßŸÜÿ®', 'ÿ¨ŸàÿßŸÜÿ®', 'ŸÅŸÜ', 'ŸÅŸÜŸàŸÜ', 'ÿ≤ÿπ€åŸÖ', 'ÿ≤ÿπŸÖÿß', 'ŸÖŸàÿ±ÿØ', 'ŸÖŸàÿßÿ±ÿØ', 'ÿ¨ÿ≤€åÿ±Ÿá', 'ÿ¨ÿ≤ÿß€åÿ±', 'ŸÅ⁄©ÿ±', 'ÿßŸÅ⁄©ÿßÿ±', 'ÿ≥ÿßŸÜÿ≠Ÿá', 'ÿ≥ŸàÿßŸÜÿ≠', 'ŸÖÿ±ÿ≠ŸÑŸá', 'ŸÖÿ±ÿßÿ≠ŸÑ', 'ÿ¨ÿ®ŸÑ', 'ÿ¨ÿ®ÿßŸÑ', 'ŸÅÿ±€åÿ∂Ÿá', 'ŸÅÿ±ÿß€åÿ∂', 'ÿ≥ŸÑÿ∑ÿßŸÜ', 'ÿ≥ŸÑÿßÿ∑€åŸÜ', 'ŸÖŸÅŸáŸàŸÖ', 'ŸÖŸÅÿßŸá€åŸÖ', 'ÿ¨ÿ±€åŸÖŸá', 'ÿ¨ÿ±ÿß€åŸÖ', 'ŸÅÿπŸÑ', 'ÿßŸÅÿπÿßŸÑ', 'ÿ¥ÿπÿ±', 'ÿßÿ¥ÿπÿßÿ±', 'ŸÖŸÜÿ®ÿπ', 'ŸÖŸÜÿßÿ®ÿπ', 'ÿ≠ÿßÿØÿ´Ÿá', 'ÿ≠ŸàÿßÿØÿ´', 'ŸÅŸÇ€åÿ±', 'ŸÅŸÇÿ±ÿß', 'ÿ¥ÿßÿπÿ±', 'ÿ¥ÿπÿ±ÿß', 'ŸÖ⁄©ÿßŸÜ', 'ÿßŸÖÿß⁄©ŸÜ', 'ÿ≠ÿ¥ŸÖ', 'ÿßÿ≠ÿ¥ÿßŸÖ', 'ŸÇÿßÿπÿØŸá', 'ŸÇŸàÿßÿπÿØ']\n",
    "mokassar_dict ={}\n",
    "for i in range(0,len(words_list),2):\n",
    "    mokassar_dict[words_list[i+1]] = words_list[i]\n",
    "                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(txt, seps):\n",
    "    default_sep = seps[0]    \n",
    "    # we skip seps[0] because that's the default separator\n",
    "    for sep in seps[1:]:\n",
    "        txt = txt.replace(sep, default_sep)\n",
    "    return [i.strip() for i in txt.split(default_sep)]\n",
    "\n",
    "#get doc and return (word,doc id)\n",
    "def tokenizer(text):\n",
    "    for l in english_char:\n",
    "        text = text.replace(l,\"\")\n",
    "    word_list = split(text.strip().replace(\"\\n\",\" \"), sep_list)\n",
    "    word_list = [x for x in word_list if not x.startswith(\"ir\") and not x.startswith(\"NID\")]\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation_marks(word_list):\n",
    "#     word_list = list(word_list)\n",
    "    for i in range(len(word_list)):\n",
    "        for mark in punctuation_marks:\n",
    "            word_list[i] = word_list[i].replace(mark,\"\")\n",
    "\n",
    "    return word_list\n",
    "\n",
    "\n",
    "def edit_long_letters(word_list):\n",
    "    patterns_compiled = [(re.compile(x[0]), x[1]) for x in raw_long_letters]\n",
    "    for pattern, rep in patterns_compiled:\n",
    "        for i in range(len(word_list)):\n",
    "            word_list[i] = pattern.sub(rep, word_list[i])\n",
    "    return word_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def remove_arabic_notation(word_list):\n",
    "    patterns_compiled = [(re.compile(x[0]), x[1]) for x in raw_arabic_notation]\n",
    "    for pattern, rep in patterns_compiled:\n",
    "        for i in range(len(word_list)):\n",
    "            word_list[i] = pattern.sub(rep, word_list[i])\n",
    "    return word_list\n",
    "\n",
    "\n",
    "def create_translation_table(src_list, dst_list):\n",
    "     return dict((ord(a), b) for a, b in zip(src_list, dst_list)) #map src unicode to dst char\n",
    "\n",
    "    \n",
    "def char_digit_Unification(word_list): \n",
    "    word_list = [x for x in word_list if not x.isnumeric() or (x.isnumeric() and float(x)<3000)]\n",
    "    translation_src, translation_dst = ' ŸâŸÉŸä‚Äú‚Äù', ' €å⁄©€å\"\"'\n",
    "    translation_src += 'ÿ¶0123456789ÿ£ÿ•ÿ¢%'\n",
    "    translation_dst += '€å€∞€±€≤€≥€¥€µ€∂€∑€∏€πÿßÿßÿßŸ™'\n",
    "    translations_table = create_translation_table(translation_src, translation_dst)\n",
    "    for i in range(len(word_list)):\n",
    "        word_list[i] = word_list[i].translate(translations_table)\n",
    "    return word_list\n",
    "\n",
    "def remove_mokassar(word_list):\n",
    "    for i in range(len(word_list)):\n",
    "        word_list[i] = mokassar_dict.get(word_list[i],word_list[i])\n",
    "        \n",
    "    return word_list\n",
    "    \n",
    "\n",
    "\n",
    "def verb_Steaming(word_list):\n",
    "    for i in range(len(word_list)):\n",
    "        word_list[i] = all_verbs.get(word_list[i],word_list[i])\n",
    "        \n",
    "    return word_list\n",
    "    \n",
    "\n",
    "def remove_postfix(word_list):\n",
    "\n",
    "    patterns_compiled = [(re.compile(x[0]), x[1]) for x in raw_postfix]\n",
    "    for pattern, rep in patterns_compiled:\n",
    "        for i in range(len(word_list)):\n",
    "            if len(word_list[i])>4 and  not (word_list[i] in all_verbs_roots):\n",
    "                word_list[i] = pattern.sub(rep, word_list[i])\n",
    "    return word_list\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def remove_prefix(word_list):\n",
    "    starts =['ÿ®€å‚Äå' , 'ŸÜÿß', 'ÿ®ÿß']\n",
    "    for i in range(len(word_list)):\n",
    "        for start in starts:\n",
    "            if word_list[i].startswith(start) and len(word_list[i])>4:\n",
    "                word_list[i] = word_list[i][len(start):]\n",
    "                break\n",
    "\n",
    "    return word_list\n",
    "\n",
    "def morakab_Unification(word_list):\n",
    "    patterns_compiled = [(re.compile(x[0]), x[1]) for x in raw_half_space]\n",
    "    for pattern, rep in patterns_compiled:\n",
    "        for i in range(len(word_list)):\n",
    "            word_list[i] = pattern.sub(rep, word_list[i])\n",
    "    return word_list\n",
    "\n",
    "#all to gether \n",
    "def normalizer(word_list):\n",
    "    #remove punc\n",
    "    for i in range(len(word_list)):\n",
    "        for mark in punctuation_marks:\n",
    "            word_list[i] = word_list[i].replace(mark,\"\")\n",
    "            \n",
    "    #edit long letters\n",
    "    patterns_compiled = [(re.compile(x[0]), x[1]) for x in raw_long_letters]\n",
    "    for pattern, rep in patterns_compiled:\n",
    "        for i in range(len(word_list)):\n",
    "            word_list[i] = pattern.sub(rep, word_list[i])\n",
    "            \n",
    "    #remove mokassar\n",
    "    for i in range(len(word_list)):\n",
    "        word_list[i] = mokassar_dict.get(word_list[i],word_list[i])\n",
    "    \n",
    "    #remove_arabic_notation\n",
    "    patterns_compiled = [(re.compile(x[0]), x[1]) for x in raw_arabic_notation]\n",
    "    for pattern, rep in patterns_compiled:\n",
    "        for i in range(len(word_list)):\n",
    "            word_list[i] = pattern.sub(rep, word_list[i])\n",
    "    \n",
    "    #char_digit_Unification\n",
    "    word_list = [x for x in word_list if not x.isnumeric() or (x.isnumeric() and float(x)<3000)]\n",
    "    translation_src, translation_dst = ' ŸâŸÉŸä‚Äú‚Äù', ' €å⁄©€å\"\"'\n",
    "    translation_src += 'ÿ¶0123456789ÿ£ÿ•ÿ¢%'\n",
    "    translation_dst += '€å€∞€±€≤€≥€¥€µ€∂€∑€∏€πÿßÿßÿßŸ™'\n",
    "    translations_table = create_translation_table(translation_src, translation_dst)\n",
    "    for i in range(len(word_list)):\n",
    "        word_list[i] = word_list[i].translate(translations_table)\n",
    "    \n",
    "    #verb_Steaming\n",
    "    for i in range(len(word_list)):\n",
    "        word_list[i] = all_verbs.get(word_list[i],word_list[i])\n",
    "        \n",
    "    #remove_prefix\n",
    "    starts =['ÿ®€å‚Äå' , 'ŸÜÿß', 'ÿ®ÿß']\n",
    "    for i in range(len(word_list)):\n",
    "        for start in starts:\n",
    "            if word_list[i].startswith(start) and len(word_list[i])>4:\n",
    "                word_list[i] = word_list[i][len(start):]\n",
    "                break\n",
    "    \n",
    "    #remove_postfix\n",
    "    patterns_compiled = [(re.compile(x[0]), x[1]) for x in raw_postfix]\n",
    "    for pattern, rep in patterns_compiled:\n",
    "        for i in range(len(word_list)):\n",
    "            if len(word_list[i])>4 and  not (word_list[i] in all_verbs_roots):\n",
    "                word_list[i] = pattern.sub(rep, word_list[i])\n",
    "    #morakab_Unification\n",
    "    patterns_compiled = [(re.compile(x[0]), x[1]) for x in raw_half_space]\n",
    "    for pattern, rep in patterns_compiled:\n",
    "        for i in range(len(word_list)):\n",
    "            word_list[i] = pattern.sub(rep, word_list[i])\n",
    "            \n",
    "    for word in word_list:\n",
    "        if word in stop_words:\n",
    "            word_list.remove(word)\n",
    "    \n",
    "    return [x for x in word_list if len(x) >= 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_data = pd.DataFrame()\n",
    "pd_list = []\n",
    "pd_list .append(pd.read_excel('IR00_3_11k News.xlsx', engine = 'openpyxl'))\n",
    "pd_list .append(pd.read_excel('IR00_3_17k News.xlsx', engine = 'openpyxl'))\n",
    "pd_list .append(pd.read_excel('IR00_3_20k News.xlsx', engine = 'openpyxl'))\n",
    "# pd_list .append(pd.read_excel('3.xlsx', engine = 'openpyxl'))\n",
    "# pd_list .append(pd.read_excel('2.xlsx', engine = 'openpyxl'))\n",
    "# pd_list .append(pd.read_excel('1.xlsx', engine = 'openpyxl'))\n",
    "for df in pd_list:\n",
    "    all_data = all_data.append(df,ignore_index=True)\n",
    "all_data[\"topic\"].replace({\"political\": \"politics\", \"sport\": \"sports\"}, inplace=True)\n",
    "all_data = all_data[all_data.content != 'None\\n\\n']\n",
    "all_data = all_data.reset_index(drop=True)\n",
    "all_data.index +=1\n",
    "all_data['id'] = all_data.index\n",
    "contents = all_data['content'].to_list()\n",
    "doc_ids = all_data['id'].to_list()\n",
    "doc_url = all_data['url']\n",
    "docs_topic = all_data['topic']\n",
    "docs_num = len(doc_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create inverted index and change docs content to bag of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dictionary(contents, doc_ids):\n",
    "    for content, id in zip(contents,doc_ids):\n",
    "        doc_tokens = tokenizer(content)\n",
    "        doc_normalized_tokens = normalizer(doc_tokens)\n",
    "        term_freq_dict = dict(collections.Counter(doc_normalized_tokens))\n",
    "        \n",
    "        for term, freq in term_freq_dict.items():\n",
    "            term_freq_dict[term] = (1 + math.log((freq), 10))\n",
    "        contents[id-1] = term_freq_dict\n",
    "        for term,freq in term_freq_dict.items():\n",
    "            term_postings = inverted_index.get(term,{})\n",
    "            term_postings[id] = (1 + math.log((freq), 10))\n",
    "            inverted_index[term] = term_postings\n",
    "\n",
    "def calculate_tf_idf_weight(contents):\n",
    "    docs_len = []\n",
    "    for doc in contents:\n",
    "        length = 0\n",
    "        for term in doc.keys():\n",
    "            df = len(inverted_index[term])\n",
    "            idf = math.log(docs_num/df,10)\n",
    "            doc[term] *= idf\n",
    "            length += doc[term]\n",
    "        docs_len.append( math.sqrt(length))\n",
    "    return docs_len\n",
    "\n",
    "            \n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "inverted_index ={}\n",
    "create_dictionary(contents, doc_ids)\n",
    "docs_len = calculate_tf_idf_weight(contents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save inverted_index and docs_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.53 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# file_inverted_index = open('ph3_KNN_inverted_index.obj', 'wb') \n",
    "# pickle.dump(inverted_index, file_inverted_index)\n",
    "# file_inverted_index.close()\n",
    "# inverted_index = {}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_docs_len = open('ph3_KNN_docs_len.obj', 'wb') \n",
    "# pickle.dump(docs_len, file_docs_len)\n",
    "# file_docs_len.close()\n",
    "# docs_len = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_contents = open('ph3_KNN_contents.obj', 'wb') \n",
    "# pickle.dump(contents, file_contents)\n",
    "# file_contents.close()\n",
    "# contents =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inverted_index['ÿ≥ŸÑÿßŸÖ']\n",
    "# docs_len =[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read inverted_index and docs_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "readed = open('ph3_KNN_inverted_index.obj', 'rb') \n",
    "inverted_index = pickle.load(readed)\n",
    "readed = open('ph3_KNN_docs_len.obj', 'rb') \n",
    "docs_len = pickle.load(readed)\n",
    "readed = open('ph3_KNN_contents.obj', 'rb') \n",
    "contents = pickle.load(readed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_inverted_index = {}\n",
    "unlabeled_df = pd.read_excel('IR_Spring2021_ph12_7k.xlsx', engine = 'openpyxl')\n",
    "unlabeled_contents = unlabeled_df['content'].to_list()\n",
    "unlabeled_doc_ids = unlabeled_df['id'].to_list()\n",
    "unlabeled_doc_url = unlabeled_df['url']\n",
    "unlabeled_docs_num = len(doc_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create unlabeled inverted_index and update unlabeled_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_unlabaled_dictionary(unlabeled_contents, unlabeled_doc_ids):\n",
    "    for content, id in zip(unlabeled_contents,unlabeled_doc_ids):\n",
    "        doc_tokens = tokenizer(content)\n",
    "        doc_normalized_tokens = normalizer(doc_tokens)\n",
    "        term_freq_dict = dict(collections.Counter(doc_normalized_tokens))\n",
    "        \n",
    "        for term, freq in term_freq_dict.items():\n",
    "            term_freq_dict[term] = (1 + math.log((freq), 10))\n",
    "            \n",
    "        unlabeled_contents[id-1] = term_freq_dict\n",
    "        for term,freq in term_freq_dict.items():\n",
    "            term_postings = unlabeled_inverted_index.get(term,{})\n",
    "            term_postings[id] = freq\n",
    "            unlabeled_inverted_index[term] = term_postings\n",
    "            \n",
    "def calculate_tf_idf_weight(unlabeled_contents):\n",
    "    unlabeled_docs_len = []\n",
    "    for doc in unlabeled_contents:\n",
    "        length = 0\n",
    "        for term in doc.keys():\n",
    "            df = len(inverted_index[term])\n",
    "            idf = math.log(docs_num/df,10)\n",
    "            doc[term] *= idf\n",
    "            length += doc[term]\n",
    "        unlabeled_docs_len.append( math.sqrt(length))\n",
    "    return unlabeled_docs_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 26.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "unlabeled_inverted_index = {}\n",
    "create_unlabaled_dictionary(unlabeled_contents, unlabeled_doc_ids)\n",
    "unlabeled_docs_len = calculate_tf_idf_weight(unlabeled_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 183 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "file_unlabeled_inverted_index = open('ph3_KNN_unlabeled_inverted_index.obj', 'wb') \n",
    "pickle.dump(unlabeled_inverted_index, file_unlabeled_inverted_index)\n",
    "file_unlabeled_inverted_index.close()\n",
    "unlabeled_inverted_index = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_unlabeled_docs_len = open('ph3_KNN_unlabeled_docs_len.obj', 'wb') \n",
    "pickle.dump(unlabeled_docs_len, file_unlabeled_docs_len)\n",
    "file_unlabeled_docs_len.close()\n",
    "unlabeled_docs_len = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_unlabeled_contents = open('ph3_KNN_unlabeled_contents.obj', 'wb') \n",
    "pickle.dump(unlabeled_contents, file_unlabeled_contents)\n",
    "file_unlabeled_contents.close()\n",
    "unlabeled_contents =[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read unlabeled inverted_index and contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 625 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "readed = open('ph3_KNN_unlabeled_inverted_index.obj', 'rb') \n",
    "unlabeled_inverted_index = pickle.load(readed)\n",
    "readed = open('ph3_KNN_unlabeled_docs_len.obj', 'rb') \n",
    "unlabeled_docs_len = pickle.load(readed)\n",
    "readed = open('ph3_KNN_unlabeled_contents.obj', 'rb') \n",
    "unlabeled_contents = pickle.load(readed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_k(doc_terms, k):\n",
    "    similarity = {}\n",
    "    docs_score = [0] * docs_num\n",
    "    for term,Wt_q in doc_terms.items():  \n",
    "        idf = math.log(docs_num/len(inverted_index[term]))\n",
    "        for doc_id, freq in inverted_index[term].items():    \n",
    "            docs_score[doc_id-1] += (Wt_q * freq)*idf\n",
    "      \n",
    "    docs_score = list((np.array(docs_score))/(np.array(docs_len)))\n",
    "    for doc_id in doc_ids:\n",
    "        if docs_score[doc_id-1] != 0:\n",
    "            similarity[doc_id] = docs_score[doc_id-1]\n",
    "    heap = [(-value, key) for key,value in similarity.items()]\n",
    "    largest = heapq.nsmallest(k, heap)\n",
    "    top_k = [(y, -x) for x,y in largest]\n",
    "    return top_k\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(unlabeled_doc_ids,unlabeled_contents, k ):\n",
    "    categories = {\"culture\":[], \"economy\":[], \"sports\":[], \"politics\":[], \"health\":[]}\n",
    "    for doc_id in tqdm(unlabeled_doc_ids):\n",
    "        doc_terms = unlabeled_contents[doc_id-1]\n",
    "        top_k = extract_top_k(doc_terms, k)\n",
    "        labels = [docs_topic[x] for x,_ in top_k]\n",
    "        categories[max(set(labels), key=labels.count)].append(doc_id)\n",
    "    return categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/7000 [00:00<?, ?it/s]C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in true_divide\n",
      "  if __name__ == '__main__':\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7000/7000 [22:02<00:00,  5.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 22min 2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cat = KNN(unlabeled_doc_ids, unlabeled_contents, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "culture    179\n",
      "economy    2019\n",
      "sports    1695\n",
      "politics    1632\n",
      "health    1475\n"
     ]
    }
   ],
   "source": [
    "for categ in cat:\n",
    "    print(categ,\"  \", len(cat[categ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save labeled docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# file_labeled_7kdocs = open('ph3_labeled_7kdocs.obj', 'wb') \n",
    "# pickle.dump(cat, file_labeled_7kdocs)\n",
    "# file_labeled_7kdocs.close()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load labeled docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.01 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "readed = open('ph3_labeled_7kdocs.obj', 'rb') \n",
    "categories = pickle.load(readed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## query responding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_query(query_str):\n",
    "    query_tokens = tokenizer(query_str)\n",
    "    query_normalized_tokens = normalizer(query_tokens)\n",
    "    term_freq_dict = dict(collections.Counter(query_normalized_tokens))\n",
    "    query_terms = {}\n",
    "    for  term,freq in term_freq_dict.items():\n",
    "        if term in inverted_index.keys() and term not in stop_words:\n",
    "            query_terms[term] = 1+math.log(freq,10)\n",
    "    print(query_terms)\n",
    "    return query_terms\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_docs(query_terms, cat_docs):\n",
    "    docs_score = {x:0 for x in cat_docs}\n",
    "    for term,Wt_q in query_terms.items():\n",
    "        for doc_id in cat_docs:\n",
    "            docs_score[doc_id] += Wt_q * unlabeled_contents[doc_id-1].get(term, 0)\n",
    "    \n",
    "    for doc_id in cat_docs:\n",
    "        docs_score[doc_id] /= unlabeled_docs_len[doc_id-1]\n",
    "    return docs_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_k(docs_similarity, k):\n",
    "\n",
    "    heap = [(-value, key) for key,value in docs_similarity.items()]\n",
    "    largest = heapq.nsmallest(k, heap)\n",
    "    top_k = [(y, -x) for x,y in largest]\n",
    "    return top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_responding():\n",
    "    query = input(\"chose one category between [culture, economy, sports, politics, health] \\n enter query like this cat:text : \")\n",
    "    category = query.split(':')[0]\n",
    "    query = query.split(':')[1]\n",
    "\n",
    "    query_terms = vectorize_query(query)\n",
    "    \n",
    "    cat_docs = categories[category]\n",
    "    docs_similarity = score_docs(query_terms, cat_docs)\n",
    "    a = datetime.datetime.now()\n",
    "    top_docs = extract_top_k(docs_similarity, 50)\n",
    "    b = datetime.datetime.now()\n",
    "\n",
    "    print(\"{:<5} result in {} ms\\nid \\t(score)\\t\\t  -> link\\n\".format(len(top_docs),1000*(b-a).total_seconds()))\n",
    "    for doc_id, score in top_docs:\n",
    "        if score>0:\n",
    "            print( \"{:<5}({}) -> {} \".format(doc_id,\"-\", doc_url[doc_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chose one category between [culture, economy, sports, politics, health] \n",
      " enter query like this cat:text : economy:ÿßŸÜŸÇŸÑÿßÿ®\n",
      "{'ÿßŸÜŸÇŸÑÿßÿ®': 1.0}\n",
      "50    result in 0.0 ms\n",
      "id \t(score)\t\t  -> link\n",
      "\n",
      "4423 (-) -> https://www.isna.ir/news/98021306870/ÿ∑ŸàŸÑ-ÿÆÿ∑Ÿàÿ∑-ÿ±€åŸÑ€å-ÿß€åÿ±ÿßŸÜ-ÿßŸÖÿ≥ÿßŸÑ-⁄ÜŸÇÿØÿ±-ŸÖ€å-ÿ¥ŸàÿØ \n",
      "3302 (-) -> https://www.isna.ir/news/99020906760/ÿßÿπÿ™ÿ®ÿßÿ±-€∑€∞-ŸÖ€åŸÑ€åÿßÿ±ÿØ€å-ÿ™Ÿàÿ≥ÿπŸá-ÿ≤€åÿ±ÿ≥ÿßÿÆÿ™-Ÿáÿß€å-⁄Øÿ±ÿØÿ¥⁄Øÿ±€å-ÿ±Ÿàÿ≥ÿ™ÿßŸáÿß€å-ÿßÿ≥ÿ™ÿßŸÜ-ÿ®Ÿàÿ¥Ÿáÿ± \n",
      "1983 (-) -> https://www.isna.ir/news/99052317223/Ÿæÿ±ŸàŸÜÿØŸá-ŸÅÿ±ŸàÿØ⁄ØÿßŸá-ŸÇÿØ€åŸÖ-ÿ∫ÿßÿ±-ŸÜŸÖ⁄©ÿØÿßŸÜ-Ÿà-ŸæŸÑÿß⁄ò-ÿ®ÿßŸÜŸàÿßŸÜ-ŸÇÿ¥ŸÖ-ÿ±Ÿà€å-ŸÖ€åÿ≤-ÿØÿßÿØÿ≥ÿ™ÿßŸÜ \n",
      "5276 (-) -> https://www.isna.ir/news/98110604396/10-Ÿáÿ≤ÿßÿ±-Ÿàÿßÿ≠ÿØ-ÿØÿ±-ÿ∑ÿ±ÿ≠-ÿßŸÇÿØÿßŸÖ-ŸÖŸÑ€å-ŸÖÿ≥⁄©ŸÜ-ÿØÿ±-ŸÇŸÖ-Ÿæ€åÿ¥-ÿ®€åŸÜ€å-ÿ¥ÿØŸá-ÿßÿ≥ÿ™ \n",
      "4186 (-) -> https://www.isna.ir/news/99111511732/€≤€∂Ÿæÿ±Ÿà⁄òŸá-ÿ®ÿß-€≤€≥€∞€∞ŸÖ€åŸÑ€åÿßÿ±ÿØ-ÿ™ŸàŸÖÿßŸÜ-ÿØÿ±-⁄©ÿ±ÿØÿ≥ÿ™ÿßŸÜ-ÿßŸÅÿ™ÿ™ÿßÿ≠-ŸÖ€å-ÿ¥ŸàÿØ \n",
      "5277 (-) -> https://www.isna.ir/news/98110604329/⁄©ŸÑŸÜ⁄Ø-4000-Ÿàÿßÿ≠ÿØ-ŸÖÿ≥⁄©ŸàŸÜ€å-ÿ∑ÿ±ÿ≠-ÿßŸÇÿØÿßŸÖ-ŸÖŸÑ€å-ÿØÿ±-ŸÇŸÖ-ÿ®ÿ±-ÿ≤ŸÖ€åŸÜ-ÿ≤ÿØŸá-ÿ¥ÿØ \n",
      "2560 (-) -> https://www.isna.ir/news/98020301754/ŸÖÿÆÿ®ÿ±-ÿØÿ¥ŸÖŸÜ-ÿ®ÿß-ÿ±Ÿàÿ≠-ÿ≠⁄©ŸàŸÖÿ™-⁄©ÿßÿ±-ÿØÿßÿ±ÿØ \n",
      "5275 (-) -> https://www.isna.ir/news/98110604422/ÿ¢€å€åŸÜ-⁄©ŸÑŸÜ⁄Ø-ÿ≤ŸÜ€å-€¥€∞€∞€∞-Ÿàÿßÿ≠ÿØ-ŸÖÿ≥⁄©ŸàŸÜ€å-ÿ∑ÿ±ÿ≠-ÿßŸÇÿØÿßŸÖ-ŸÖŸÑ€å-ÿØÿ±-ÿßÿ≥ÿ™ÿßŸÜ-ŸÇŸÖ \n",
      "4504 (-) -> https://www.isna.ir/news/98022714140/ÿßÿÆÿ™ÿµÿßÿµ-€≤€µ€∞€∞-ŸÖ€åŸÑ€åÿßÿ±ÿØ-ÿ™ŸàŸÖÿßŸÜ-ŸàÿßŸÖ-ŸÇÿ±ÿ∂-ÿßŸÑÿ≠ÿ≥ŸÜŸá-ÿ≥ÿßÿÆÿ™-Ÿà-ÿ™ÿπŸÖ€åÿ±-ŸÖÿ≥⁄©ŸÜ-ÿ®ÿ±ÿß€å \n",
      "5298 (-) -> https://www.isna.ir/news/98111107354/99-5-ÿØÿ±ÿµÿØ-ÿßÿ≤-ÿßÿ≥ÿ™ÿßŸÜ-ŸÇŸÖ-ÿ™ÿ≠ÿ™-ŸæŸàÿ¥ÿ¥-⁄Øÿßÿ≤ÿ±ÿ≥ÿßŸÜ€å-ŸÇÿ±ÿßÿ±-ÿØÿßÿ±ÿØ \n",
      "4360 (-) -> https://www.isna.ir/news/98012711914/ÿÆÿ≥ÿßÿ±ÿ™-ÿ≥€åŸÑ-ÿ®Ÿá-€±€¥€±-ÿ±Ÿàÿ≥ÿ™ÿß-Ÿà-ÿ¥Ÿáÿ±-ÿßÿ≥ÿ™ÿßŸÜ-€åÿ≤ÿØ \n",
      "4159 (-) -> https://www.isna.ir/news/99110503344/⁄©ÿØÿßŸÖ-ÿ®ÿÆÿ¥-Ÿáÿß€å-ÿßŸÇÿ™ÿµÿßÿØ-Ÿæÿ≥-ÿßÿ≤-⁄©ÿ±ŸàŸÜÿß-ÿßŸàÿ¨-ŸÖ€å-⁄Ø€åÿ±ŸÜÿØ \n",
      "3503 (-) -> https://www.isna.ir/news/99040403412/ÿ¥€åÿ®-⁄©ÿßŸáÿ¥€å-Ÿàÿßÿ±ÿØÿßÿ™-⁄©ÿßŸÑÿß-ÿßÿ≤-⁄ØŸÖÿ±⁄©ÿßÿ™-ÿ®Ÿàÿ¥Ÿáÿ± \n",
      "4326 (-) -> https://www.isna.ir/news/98011907014/ÿßŸÑŸÇÿß€å-ÿ¨Ÿà-ÿ±ŸàÿßŸÜ€å-ÿ®ÿ±ÿß€å-ÿßŸÅÿ≤ÿß€åÿ¥-ŸÇ€åŸÖÿ™-ÿßÿ±ÿ≤ \n",
      "2587 (-) -> https://www.isna.ir/news/98021005645/ÿπÿßÿ±ŸÅ-ÿ®€åÿ¥ÿ™ÿ±€åŸÜ-ŸÅÿ¥ÿßÿ±-ÿßŸÇÿ™ÿµÿßÿØ€å-ŸÖÿ™Ÿàÿ¨Ÿá-⁄©ÿßÿ±⁄Øÿ±ÿßŸÜ-ÿßÿ≥ÿ™ \n",
      "5121 (-) -> https://www.isna.ir/news/98090704741/Ÿáÿ±-⁄©ÿØÿßŸÖ-ÿßÿ≤-ÿ™ÿ≠ÿ±€åŸÖ-Ÿáÿß€å-ÿßŸÇÿ™ÿµÿßÿØ€å-ŸÖ€å-ÿ™ŸàÿßŸÜÿØ-ÿØŸàŸÑÿ™€å-ÿ±ÿß-ÿ≥ÿ±ŸÜ⁄ØŸàŸÜ-⁄©ŸÜÿØ \n",
      "4409 (-) -> https://www.isna.ir/news/98020804058/⁄Üÿ±ÿß-ÿ™ÿπÿßŸàŸÜ€å-Ÿáÿß-ŸÇÿØÿ±ÿ™-ŸÇÿØ€åŸÖ-ÿ±ÿß-ŸÜÿØÿßÿ±ŸÜÿØ \n",
      "3740 (-) -> https://www.isna.ir/news/99061108330/ÿ™ŸÇŸà€åÿ™-ÿ±Ÿàÿßÿ®ÿ∑-ÿ™ÿ¨ÿßÿ±€å-ÿß€åÿ±ÿßŸÜ-Ÿà-⁄©ÿ±Ÿá-ÿ¥ŸÖÿßŸÑ€å \n",
      "3219 (-) -> https://www.isna.ir/news/99011607964/ŸæÿßŸÑÿß€åÿ¥-ŸÖÿ™ŸÇÿßÿ∂€åÿßŸÜ-ÿß€åŸÜÿ™ÿ±ŸÜÿ™€å-ŸÖÿ≥⁄©ŸÜ-ŸÖŸÑ€å-ÿ¢ÿ∫ÿßÿ≤-ÿ¥ÿØ \n",
      "3520 (-) -> https://www.isna.ir/news/99041309682/ÿ™ŸàŸÑ€åÿØ-L€π€∞-ÿß€åÿ±ÿßŸÜ€å-ÿØÿ±-ÿ≠ÿßŸÑ-Ÿæ€å⁄Ø€åÿ±€å-ÿßÿ≥ÿ™ \n",
      "2996 (-) -> https://www.isna.ir/news/98082717828/ŸÜŸÖ€å-ÿÆŸàÿßŸá€åŸÖ-ÿØÿ±-ÿß€åŸÜ-ÿ¥ÿ±ÿß€åÿ∑-⁄ÜŸÖÿßŸÇ-ÿ±Ÿà€å-ÿ≥ÿ±-ÿßŸÅÿ±ÿßÿØ-ÿ®⁄Ø€åÿ±€åŸÖ \n",
      "2762 (-) -> https://www.isna.ir/news/98050502261/ÿ∑ŸÑÿ®⁄©ÿßÿ±ÿßŸÜ-ŸæÿØ€åÿØŸá-ÿ®Ÿá-ÿ≥ŸáÿßŸÖÿØÿßÿ±ÿßŸÜ-ÿ™ÿ®ÿØ€åŸÑ-ÿ¥ÿØŸÜÿØ \n",
      "3840 (-) -> https://www.isna.ir/news/99071712978/ÿ±ŸàÿßŸÜ-ÿ®ÿÆÿ¥€å-ÿ±Ÿàÿ≥ÿ™ÿßŸáÿß-ÿ®ÿß-ÿ∑ÿ±ÿ≠-ŸáÿßÿØ€å \n",
      "5222 (-) -> https://www.isna.ir/news/98100906510/ŸÖÿµŸàÿ®Ÿá-ÿØŸàŸÑÿ™-ÿØÿ±-ÿ™ÿßŸÖ€åŸÜ-80-Ÿàÿß⁄ØŸÜ-Ÿæ€åÿ¥ÿ®ÿ±ÿØ-ŸÖÿ™ÿ±Ÿà€å-ŸÇŸÖ-ÿ±ÿß-ÿ™ÿ≥Ÿá€åŸÑ-⁄©ÿ±ÿØ \n",
      "5160 (-) -> https://www.isna.ir/news/98092015527/ÿßŸÜÿπŸÇÿßÿØ-ŸÇÿ±ÿßÿ±ÿØÿßÿØ-ÿØÿßÿÆŸÑ€å-ÿ≥ÿßÿ≤€å-ŸÇÿ∑ÿπÿßÿ™-ÿµŸÜÿπÿ™-ÿÆŸàÿØÿ±Ÿà-ÿ®Ÿá-ÿßÿ±ÿ≤ÿ¥-€≤€∑€µ-ŸÖ€åŸÑ€åŸàŸÜ \n",
      "4148 (-) -> https://www.isna.ir/news/99110201450/ÿÆŸàÿØÿ±Ÿà€å-ÿØÿßÿÆŸÑ€å-ÿ™ÿß-€≥€∞-ÿØÿ±ÿµÿØ-ÿßŸÅÿ™-ŸÇ€åŸÖÿ™-ÿØÿßÿ¥ÿ™Ÿá-ÿßÿ≥ÿ™ \n",
      "3387 (-) -> https://www.isna.ir/news/99022920827/ÿ≥ŸáŸÖ-€∂-€µ-ÿØÿ±ÿµÿØ€å-ÿßŸÇÿ™ÿµÿßÿØ-ÿØ€åÿ¨€åÿ™ÿßŸÑ-ÿßÿ≤-ÿ™ŸàŸÑ€åÿØ-ŸÜÿßÿÆÿßŸÑÿµ-ÿØÿßÿÆŸÑ€å \n",
      "4552 (-) -> https://www.isna.ir/news/98030803635/ÿ¨ÿßÿØŸá-ÿß€å-⁄©Ÿá-ÿßÿ≤-ÿ≥ÿßŸÑ-€∂€≤-ÿ™ÿß⁄©ŸÜŸàŸÜ-ŸÜ€åŸÖŸá-⁄©ÿßÿ±Ÿá-ŸÖÿßŸÜÿØŸá-ÿßÿ≥ÿ™ \n",
      "4821 (-) -> https://www.isna.ir/news/98052713445/ÿßŸÖÿ±Ÿàÿ≤-ÿ®ÿßÿ≤ÿßÿ±-ÿßŸÜÿ®ÿßÿ¥ÿ™Ÿá-ÿßÿ≤-ÿßŸÇŸÑÿßŸÖ-ŸÖÿÆÿ™ŸÑŸÅ-ÿßÿ≥ÿ™ \n",
      "4874 (-) -> https://www.isna.ir/news/98061106058/ÿßÿ¨ÿ±ÿß€å-ÿ∑ÿ±ÿ≠-€åÿßÿ±ÿßŸÜŸá-ÿØÿ≥ÿ™ŸÖÿ≤ÿØ-⁄Øÿ¥ÿß€åÿ¥€å-ÿ¨ÿØ€åÿØ-ÿØÿ±-ÿßŸÖÿ±-ÿßÿ¥ÿ™ÿ∫ÿßŸÑ \n",
      "4087 (-) -> https://www.isna.ir/news/99101008022/ŸÖŸÇÿßŸàŸÖ-ÿ≥ÿßÿ≤€å-€¥€≥-Ÿáÿ≤ÿßÿ±-Ÿà-€∏€π€∞-Ÿàÿßÿ≠ÿØ-ÿØÿ±-ÿ≥ÿ∑ÿ≠-ÿßÿ≥ÿ™ÿßŸÜ-ÿ®Ÿàÿ¥Ÿáÿ± \n",
      "4048 (-) -> https://www.isna.ir/news/99093023274/ÿ®ÿ±ÿß€å-ÿ™⁄©ŸÖ€åŸÑ-ŸÖŸàÿ¨ŸàÿØ€å-ŸÖÿ≥⁄©ŸÜ-ŸÖŸÑ€å-ŸÅŸÇÿ∑-ÿßŸÖÿ±Ÿàÿ≤-ŸÅÿ±ÿµÿ™-ÿØÿßÿ±€åÿØ \n",
      "4847 (-) -> https://www.isna.ir/news/98060301355/ÿ™ŸÅÿßŸàÿ™-Ÿá€åŸàŸÜÿØÿß-Ÿà-ÿß€åÿ±ÿßŸÜ-ÿÆŸàÿØÿ±Ÿà-ÿßÿ≤-ŸÜ⁄ØÿßŸá-ÿπÿ∂Ÿà-ÿßÿ™ÿßŸÇ-ÿ®ÿßÿ≤ÿ±⁄ØÿßŸÜ€å-ŸÖÿ¥ŸáÿØ \n",
      "4624 (-) -> https://www.isna.ir/news/98033014767/ÿßÿ±ÿßÿ¶Ÿá-ÿ∑ÿ±ÿ≠-ÿßÿµŸÑÿßÿ≠-ÿ≥ÿßÿÆÿ™ÿßÿ±-ÿ®ŸàÿØÿ¨Ÿá-⁄©ÿ¥Ÿàÿ±-ÿØÿ±-ÿ¨ŸÑÿ≥Ÿá-ÿ≥ÿ±ÿßŸÜ-ŸÇŸàÿß \n",
      "5290 (-) -> https://www.isna.ir/news/98110705275/ÿ™ÿßÿ¨-⁄Øÿ±ÿØŸàŸÜ-ÿ≠ŸÇŸàŸÇ-€å⁄©-ŸÖ€åŸÑ€åŸàŸÜ-Ÿà-€±€∞€∞-Ÿáÿ≤ÿßÿ±-⁄©ÿßÿ±ŸÖŸÜÿØ-ÿØÿ±-ÿ≥ÿßŸÑ-ÿ¢€åŸÜÿØŸá-€∏€∞-ÿØÿ±ÿµÿØ \n",
      "4615 (-) -> https://www.isna.ir/news/98032813583/ÿ¢ÿ∫ÿßÿ≤-ŸÜŸáÿ∂ÿ™-ÿÆÿßŸÜŸá-ÿ≥ÿßÿ≤€å-ÿØÿ±-Ÿàÿ≤ÿßÿ±ÿ™-ÿ±ÿßŸá-Ÿà-ÿ¥Ÿáÿ±ÿ≥ÿßÿ≤€å \n",
      "1933 (-) -> https://www.isna.ir/news/99050402405/ÿ¢ŸÖÿ±€å⁄©ÿß€å€å-Ÿáÿß-ŸÇÿµÿØ-ÿ≥ÿ±ŸÜ⁄ØŸàŸÜ€å-ŸáŸàÿßŸæ€åŸÖÿß€å-ŸÖÿ≥ÿßŸÅÿ±ÿ®ÿ±€å-ÿß€åÿ±ÿßŸÜ-ÿ±ÿß-ÿØÿßÿ¥ÿ™ŸÜÿØ \n",
      "4914 (-) -> https://www.isna.ir/news/98062009860/66-ÿØÿ±ÿµÿØ-ÿØÿ¥ÿ™-Ÿáÿß€å-ÿß€åÿ±ÿßŸÜ-ŸÖŸÖŸÜŸàÿπŸá-ÿßŸÜÿØ-10-Ÿáÿ≤ÿßÿ±-ÿ±Ÿàÿ≥ÿ™ÿß-ÿ¢ÿ®-Ÿæÿß€åÿØÿßÿ±-ŸÜÿØÿßÿ±ŸÜÿØ \n",
      "4418 (-) -> https://www.isna.ir/news/98021106309/ÿßŸÖ⁄©ÿßŸÜ-ÿ™ÿÆÿµ€åÿµ-€µ-ŸÖ€åŸÑ€åÿßÿ±ÿØ-ÿ™ŸàŸÖÿßŸÜ-ÿ®Ÿá-Ÿæÿ±Ÿà⁄òŸá-Ÿáÿß€å-ÿ¢ÿ®-ÿ±ÿ≥ÿßŸÜ€å-ÿπÿ¥ÿß€åÿ±-ŸÇÿ≤Ÿà€åŸÜ \n",
      "4713 (-) -> https://www.isna.ir/news/98042613967/ÿ≥ŸáŸÖ-ŸÜÿß⁄Ü€åÿ≤-ÿß€åÿ±ÿßŸÜ-ÿßÿ≤-ÿ®ÿßÿ≤ÿßÿ±-€≤€µ€∞€∞-ŸÖ€åŸÑ€åÿßÿ±ÿØ-ÿØŸÑÿßÿ±€å-ÿµÿßÿØÿ±ÿßÿ™-ÿ≠ŸÑÿßŸÑ \n",
      "4873 (-) -> https://www.isna.ir/news/98061005228/ÿ®ÿ±ŸÜÿØŸáÿß€å-ÿ®€åŸÜ-ÿßŸÑŸÖŸÑŸÑ€å-⁄©ÿ¥Ÿàÿ±-ÿßŸÜ⁄Øÿ¥ÿ™-ÿ¥ŸÖÿßÿ±-ÿßÿ≥ÿ™ \n",
      "4913 (-) -> https://www.isna.ir/news/98062009861/ÿ™ŸÜŸáÿß-5-ÿØÿ±ÿµÿØ-ŸÇÿ∑ÿßÿ±Ÿáÿß€å-ÿß€åÿ±ÿßŸÜ-ŸæŸÜÿ¨-ÿ≥ÿ™ÿßÿ±Ÿá-ÿßŸÜÿØ \n",
      "4995 (-) -> https://www.isna.ir/news/98071612661/ÿßŸÜÿ¨ŸÖŸÜ-Ÿáÿß€å-ÿµŸÜŸÅ€å-ÿÆŸàÿßÿ≥ÿ™ÿßÿ±-ÿ™ŸàŸÇŸÅ-ÿ™ÿµŸà€åÿ®-ŸÑÿß€åÿ≠Ÿá-ÿ™ÿ¨ÿßÿ±ÿ™-ÿ¥ÿØŸÜÿØ \n",
      "4726 (-) -> https://www.isna.ir/news/98043015879/ÿßŸàŸÑ€åŸÜ-ŸÖÿ∑ÿßŸÑÿ®Ÿá-ÿßÿ™ÿßŸÇ-ÿß€åÿ±ÿßŸÜ-ÿ¥ŸÅÿßŸÅ€åÿ™-Ÿà-ŸÖŸÇÿßÿ®ŸÑŸá-ÿ®ÿß-ŸÅÿ≥ÿßÿØ-ÿßÿ≥ÿ™ \n",
      "2681 (-) -> https://www.isna.ir/news/98040100590/⁄©ÿßŸáÿ¥-€±€¥-ÿØÿ±ÿµÿØ€å-ÿ¢ŸÖÿßÿ±-ÿßÿ≤ÿØŸàÿßÿ¨-Ÿà-ÿ∑ŸÑÿßŸÇ-ÿØÿ±-ÿßÿ≥ÿ™ÿßŸÜ-ŸÖÿ±⁄©ÿ≤€å \n",
      "5300 (-) -> https://www.isna.ir/news/98111208137/ÿß€åÿ¨ÿßÿØ-2074-ÿ¥ÿ∫ŸÑ-ÿ±Ÿá-ÿ¢Ÿàÿ±ÿØ-ÿßŸÅÿ™ÿ™ÿßÿ≠-Ÿæÿ±Ÿà⁄òŸá-Ÿáÿß€å-ÿØŸáŸá-ŸÅÿ¨ÿ±-98-ŸÇŸÖ \n",
      "5380 (-) -> https://www.isna.ir/news/98121814020/ÿ¢ÿ∫ÿßÿ≤-ÿ´ÿ®ÿ™-ŸÜÿßŸÖ-ŸÖÿ≥⁄©ŸÜ-ŸÖŸÑ€å-ÿØÿ±-€±€∑-ÿßÿ≥ÿ™ÿßŸÜ-ÿßÿ≤-ŸÅÿ±ÿØÿß-ÿ¥ÿ±Ÿàÿ∑-ÿ¨ÿØ€åÿØ€å-ÿßÿ∂ÿßŸÅŸá-ÿ¥ÿØ \n",
      "5136 (-) -> https://www.isna.ir/news/98091208462/ÿ®ÿß-ÿ≥€åÿßÿ≥€å-⁄©ÿ±ÿØŸÜ-ÿ™ÿ±ÿßÿ±€åÿÆÿ™Ÿá-Ÿáÿß-Ÿæ€åÿ¥ÿ±ŸÅÿ™-ÿπŸÑŸÖ€å-⁄©ÿ¥Ÿàÿ±-ÿ∞ÿ®ÿ≠-ŸÖ€å-ÿ¥ŸàÿØ-ÿ®ÿ≠ÿ´-ÿπŸÑ€åŸá \n"
     ]
    }
   ],
   "source": [
    "query_responding()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
